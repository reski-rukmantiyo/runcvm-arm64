#!/opt/runcvm/lib/ld-musl-aarch64.so.1 /opt/runcvm/bin/bash

# DIAGNOSTIC: Dump environment to check for RUNCVM_LOG_LEVEL leakage
env | grep RUNCVM > /tmp/runcvm-env-leak.log || true

# REFERENCES

# Virtiofs
# - https://vmsplice.net/~stefan/virtio-fs_%20A%20Shared%20File%20System%20for%20Virtual%20Machines.pdf

# Container config.json spec
# - https://github.com/opencontainers/runtime-spec/
# - https://github.com/opencontainers/runtime-spec/blob/main/config.md

# Mount namespaces
# - https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt
# - https://www.redhat.com/sysadmin/mount-namespaces

RUNCVM=${RUNCVM:-/opt/runcvm}
if [ -x "$RUNCVM/lib/ld" ]; then
  RUNCVM_LD=$RUNCVM/lib/ld
else
  RUNCVM_LD=""
fi

if [ -x "$RUNCVM/usr/bin/jq" ]; then
  RUNCVM_JQ=$RUNCVM/usr/bin/jq
else
  RUNCVM_JQ="jq"
fi
RUNCVM_VM_MOUNTPOINT="/vm"
RUNCVM_GUEST=/.runcvm/guest
RUNCVM_ENTRYPOINT=$RUNCVM_GUEST/scripts/runcvm-ctr-entrypoint
RUNCVM_EXEC="$RUNCVM_GUEST/scripts/runcvm-ctr-exec"
RUNCVM_KERNELS=$RUNCVM/kernels
RUNCVM_GUEST_KERNELS=$RUNCVM_GUEST/kernels
RUNCVM_KERNEL_DEFAULT=firecracker
RUNCVM_MEM_SIZE_DEFAULT="768" # Default VM memory size (Mb) if not specified in container config
RUNCVM_CONTAINER_MEM_OVERHEAD="256" # Memory (Mb) overhead to allow for Firecracker, virtiofsd, dnsmasq and other container memory footprint: 768Mb minimum, 1024Mb recommended

# ============================================================
# LOGGING SYSTEM
# Severity levels: DEBUG, INFO, ERROR, OFF
# Control via: RUNCVM_LOG_LEVEL environment variable
# ============================================================

# Configure Logger for Runtime
RUNCVM_COMPONENT_NAME="RunCVM-Runtime"
RUNCVM_LOG_FILE="/opt/runcvm/runtime.log"

# Source logging functions
. "$RUNCVM/scripts/common/logging.sh"


# ============================================================
# CLEANUP HANDLER
# Ensures unfsd is stopped when runtime is terminated
# ============================================================

# Store container ID globally for cleanup
CLEANUP_CONTAINER_ID=""

cleanup_on_exit() {
  if [ -n "$CLEANUP_CONTAINER_ID" ] && [ -x /opt/runcvm/scripts/runcvm-nfsd ]; then
    echo "[CLEANUP] Stopping all unfsd instances for container $CLEANUP_CONTAINER_ID" >&2
    /opt/runcvm/scripts/runcvm-nfsd stop-all "$CLEANUP_CONTAINER_ID" 2>&1 || true
  fi
}

# Register cleanup handlers for signals
trap cleanup_on_exit EXIT SIGTERM SIGINT

# Source JSON helper functions
. "$RUNCVM/scripts/common/json.sh"


# is_sandbox_container - Detect Kubernetes sandbox/pause containers
# These should bypass RunCVM and run directly with runc
# Returns 0 if sandbox container detected, 1 otherwise
is_sandbox_container() {
  local config_file="$1"
  
  # Check for Kubernetes CRI sandbox annotation
  # containerd sets this annotation for pause containers
  if jq_get "$config_file" '.annotations["io.kubernetes.cri.container-type"]' 2>/dev/null | grep -q "sandbox"; then
    log "Detected sandbox container via annotation io.kubernetes.cri.container-type=sandbox"
    return 0
  fi
  
  # Check for pause in image name (annotations may contain image info)
  local image_name=$(jq_get "$config_file" '.annotations["io.kubernetes.cri.image-name"]' 2>/dev/null)
  if echo "$image_name" | grep -qiE "(^|/)pause[:@]|/mirrored-pause[:@]"; then
    log "Detected sandbox container via image name: $image_name"
    return 0
  fi
  
  # Check process args for /pause binary
  local arg0=$(jq_get "$config_file" '.process.args[0]' 2>/dev/null)
  if [ "$arg0" = "/pause" ]; then
    log "Detected sandbox container via entrypoint: /pause"
    return 0
  fi
  
  # Check root path for pause-related paths (containerd naming)
  local root_path=$(jq_get "$config_file" '.root.path' 2>/dev/null)
  if echo "$root_path" | grep -qE "pause"; then
    log "Detected sandbox container via root path: $root_path"
    local boot_args="init=/init console=ttyS0 reboot=k panic=1 pci=off root=/dev/vda rw"
    # Add 'quiet' to suppress kernel boot messages when not in DEBUG mode
    if [ "$RUNCVM_LOG_LEVEL" != "DEBUG" ]; then
      boot_args="$boot_args quiet"
    fi
    return 0
  fi

  # Check for io.kubernetes.cri.sandbox-id being equal to container id
  # (sandbox containers have their own id as sandbox-id)
  local sandbox_id=$(jq_get "$config_file" '.annotations["io.kubernetes.cri.sandbox-id"]' 2>/dev/null)
  local container_id=$(jq_get "$config_file" '.annotations["io.kubernetes.cri.container-id"]' 2>/dev/null)
  if [ -n "$sandbox_id" ] && [ "$sandbox_id" = "$container_id" ] && [ "$sandbox_id" != "null" ]; then
    log "Detected sandbox container via matching sandbox-id and container-id: $sandbox_id"
    return 0
  fi
  
  return 1
}

# PARSE RUNC GLOBAL OPTIONS:
# --debug             enable debug logging
# --log value         set the log file to write runc logs to (default is '/dev/stderr')
# --log-format value  set the log format ('text' (default), or 'json') (default: "text")
# --root value        root directory for storage of container state (this should be located in tmpfs) (default: "/run/user/1000/runc")
# --criu value        path to the criu binary used for checkpoint and restore (default: "criu")
# --systemd-cgroup    enable systemd cgroup support, expects cgroupsPath to be of form "slice:prefix:name" for e.g. "system.slice:runc:434234"
# --rootless value    ignore cgroup permission errors ('true', 'false', or 'auto') (default: "auto")

COMMAND_LINE=("$@")

log_debug "Command line: $0 ${COMMAND_LINE[@]@Q}"

while true
do
  case "$1" in
    --debug|--systemd-cgroup) shift; continue; ;;
    --log|--log-format|--root|--criu|--rootless) shift; shift; continue; ;;
    --log=*|--log-format=*|--root=*|--criu=*|--rootless=*) shift; continue; ;;
    *) break; ;;
  esac
done

COMMAND="$1"
shift

if [ "$COMMAND" = "create" ]; then

  log_debug "Command: create"
  
  # USAGE:
  #    runc create [command options] <container-id>
  #   
  # PARSE 'create' COMMAND OPTIONS
  # --bundle value, -b value  path to the root of the bundle directory, defaults to the current directory
  # --console-socket value    path to an AF_UNIX socket which will receive a file descriptor referencing the master end of the console's pseudoterminal
  # --pid-file value          specify the file to write the process id to
  # --no-pivot                do not use pivot root to jail process inside rootfs.  This should be used whenever the rootfs is on top of a ramdisk
  # --no-new-keyring          do not create a new session keyring for the container.  This will cause the container to inherit the calling processes session key
  # --preserve-fds value      Pass N additional file descriptors to the container (stdio + $LISTEN_FDS + N in total) (default: 0)
  while true
  do
    case "$1" in
      --bundle|-b) shift; BUNDLE="$1"; shift; continue; ;;
      --console-socket|--pid-file|--preserve-fds) shift; shift; continue; ;;
      --no-pivot|--no-new-keyring) shift; continue; ;;
      *) break; ;;
    esac
  done

  ID="$1"
  
  # Store ID for cleanup handler (in case of SIGTERM/SIGINT)
  CLEANUP_CONTAINER_ID="$ID"
  
  # SETUP SOCKET DIRECTORY FOR API
  # This allows host to communicate with Firecracker inside container
  RUNCVM_SOCKET_DIR="/var/lib/runcvm/sockets/$ID"
  mkdir -p "$RUNCVM_SOCKET_DIR"
  chmod 700 "$RUNCVM_SOCKET_DIR"

  CFG="$BUNDLE/config.json"
  ROOT=$(jq -r .root.path $CFG)

  # Allow user to enable debug logging
  if [ "$(get_config_env RUNCVM_RUNTIME_DEBUG)" = "1" ]; then
    RUNCVM_LOG_LEVEL="DEBUG"
    CURRENT_LOG_LEVEL=$(get_log_level_num "DEBUG")
  fi

  log_debug "Command line: $0 ${COMMAND_LINE[@]@Q}"
  log_debug "Command: create bundle=$BUNDLE id=$ID root=$ROOT"
  
  # Save formatted config.json
  jq -r . <$CFG >/tmp/config.json-$$-1

  # ============================================================
  # SANDBOX CONTAINER DETECTION
  # If this is a Kubernetes sandbox/pause container, bypass RunCVM
  # and pass directly to runc without modification
  # ============================================================
  if is_sandbox_container "$CFG"; then
    log "Sandbox container detected - bypassing RunCVM, passing to runc"
    exec /usr/bin/runc "${COMMAND_LINE[@]}"
  fi
  # ============================================================
  
  # Pending support for user-specified mountpoint for the guest (VM) binaries and scripts
  set_config_env "RUNCVM_GUEST" "$RUNCVM_GUEST"

  ARG0=$(jq_get "$CFG" '.process.args[0]')
  # Now look in mounts for destination == $ARG0 (this works for Docker and Podman)
  if [ "$ARG0" = "/sbin/docker-init" ] || [ "$ARG0" = "/dev/init" ]; then
  
    # User intended an init process to be run in the container,
    # so arrange to run our own instead, that will launch the original entrypoint
    
    # Look for and remove a mountpoint for this process.
    jq_set "$CFG" --arg init "$ARG0" '(.mounts[] | select(.destination == $init)) |= empty'
    
    # Replace the first argument with our own entrypoint; and remove the second, '--' (for now, #TODO)
    jq_set "$CFG" --arg entrypoint "$RUNCVM_ENTRYPOINT" '.process.args[0] = $entrypoint | del(.process.args[1])'
    
    # We know the user intended an init process to be run in the container.
    # TODO: We might want to indicate this, so that our entrypoint does not skip doing this
    # if the original entrypoint also looks like an init process.
    set_config_env "RUNCVM_INIT" "1"
  else
    # We don't know if the original entrypoint is an init process or not.
    # Run our entrypoint first to work this out and do the right thing.
    
    jq_set "$CFG" --arg entrypoint "$RUNCVM_ENTRYPOINT" '.process.args |= [$entrypoint] + .'
  fi

  # SET RUNCVM_HAS_HOME
  # 
  # If the HOME env var was not set either in the image, or via docker run, 
  # then it will be missing in the config env. Detect this case for communication to runcvm-ctr-entrypoint
  # so that HOME can be set to the requested user's default homedir.
  #
  # - See runcvm-ctr-entrypoint for full details of how/why hasHome is needed and HOME gets set.
  if [ -n "$(get_config_env HOME)" ]; then
    set_config_env "RUNCVM_HAS_HOME" "1"
  else
    set_config_env "RUNCVM_HAS_HOME" "0"
  fi

  # CONFIGURE USER
  # - Must be root to run container
  RUNCVM_UIDGID=$(jq_get "$CFG" '(.process.user.uid | tostring) + ":" + (.process.user.gid | tostring) + ":" + ((.process.user.additionalGids // []) | join(","))')
  set_config_env "RUNCVM_UIDGID" "$RUNCVM_UIDGID"
  jq_set "$CFG" '.process.user = {"uid":0, "gid":0}'
  log "RUNCVM_UIDGID=$RUNCVM_UIDGID"

  # CONFIGURE CPUS
  # CONFIGURE CPUS
  # Calculate CPUs based on quota and period
  # Default period is 100000 (100ms) if not specified
  CPU_QUOTA=$(jq_get "$CFG" '.linux.resources.cpu.quota')
  CPU_PERIOD=$(jq_get "$CFG" '.linux.resources.cpu.period // 100000')

  if [ "$CPU_QUOTA" = "null" ] || [ "$CPU_QUOTA" = "-1" ]; then
    # Unlimited CPUs
    RUNCVM_CPUS="0"
  else
    # Calculate ceil(quota / period) to ensure fractional CPUs get at least 1 vCPU
    # and e.g. 1.5 CPUs gets 2 vCPUs
    RUNCVM_CPUS=$(( ($CPU_QUOTA + $CPU_PERIOD - 1) / $CPU_PERIOD ))
    # Ensure at least 1 vCPU
    [ "$RUNCVM_CPUS" -lt 1 ] && RUNCVM_CPUS=1
  fi
  
  set_config_env "RUNCVM_CPUS" "$RUNCVM_CPUS"
  log "CPU Config: Quota=$CPU_QUOTA Period=$CPU_PERIOD -> RUNCVM_CPUS=$RUNCVM_CPUS"

  # CONFIGURE MOUNTS
  set_config_env "RUNCVM_VM_MOUNTPOINT" "$RUNCVM_VM_MOUNTPOINT"

  # First extract list of tmpfs mounts in fstab form, then delete them from the config
  RUNCVM_TMPFS=$(jq_get "$CFG" '( .mounts[] | select(.type == "tmpfs" and (.destination | test("^/dev(/|$)") | not) ) ) | [.source + " " + .destination + " tmpfs " + (.options | map(select(. != "rprivate" and . != "private")) | join(",")) + " 0 0"] | .[0]')
  jq_set "$CFG" -r 'del( .mounts[] | select(.type == "tmpfs" and (.destination | test("^/dev(/|$)") | not) ) )'
  set_config_env "RUNCVM_TMPFS" "$RUNCVM_TMPFS"

  # Rewrite all pre-existing bind/volume mounts (except those at or below /disks) to mount
  # below $RUNCVM_VM_MOUNTPOINT instead of below /.
  #
  # TODO TO CONSIDER:
  # If we excluded /etc/(resolv.conf,hosts,hostname), and moved these to top of the array
  # (by promoting them at the end of the below statements), they would be present in both
  # container and VM.
  #
  # N.B. A mount at or underneath /disks will NOT be mapped to /vm/disks - this path is reserved for mounting disk files to the container
  jq_set "$CFG" --arg vm "$RUNCVM_VM_MOUNTPOINT" '( .mounts[] | select(.type == "bind" and (.destination | test("^/disks(/|$)") | not) ) ).destination |= $vm + .'
  # ==========================================================================
  # CONFIGURE NFS VOLUMES
  # ==========================================================================
  log "DEBUG: Checking RUNCVM_HYPERVISOR env..."
  HYPERVISOR="$(get_config_env RUNCVM_HYPERVISOR firecracker)"
  log "DEBUG: HYPERVISOR=$HYPERVISOR"
  log "DEBUG: ROOT=$ROOT"
  
  # Pass RUNCVM_NETWORK_MODE if present
  RUNCVM_NETWORK_MODE="$(get_config_env RUNCVM_NETWORK_MODE)"
  if [ -n "$RUNCVM_NETWORK_MODE" ]; then
    set_config_env "RUNCVM_NETWORK_MODE" "$RUNCVM_NETWORK_MODE"
  fi
  
  if [ "$HYPERVISOR" = "firecracker" ]; then
    log "FIRECRACKER: Creating NFS mounts..."
    [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ] && echo "[DEBUG] FIRECRACKER: Creating NFS mounts..." >&2
    [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ] && echo "[DEBUG] ROOT=$ROOT" >&2
    
    # Firecracker Rootfs Caching
    # Logic:
    # 1. Generate a cache key based on the rootfs path (Docker overlay2 merged directory).
    #    This path is unique to the specific set of image layers.
    # 2. Mount the host cache directory (/var/lib/runcvm/cache) to the container (/.runcvm/cache).
    # 3. Pass the key to the container env.

    # 1. Generate Key
    if [ -n "$ROOT" ] && [ -d "$ROOT" ]; then
      CACHE_SOURCE_STRING="$ROOT"
      
      # Try to get stable key from overlay lowerdir (which represents the image)
      # This is stable across different container instances of the same image
      if command -v findmnt >/dev/null 2>&1; then
         # Extract lowerdir from options
         # findmnt might not be in path, check /usr/bin etc if needed, but command -v handles PATH
         LOWERDIR=$(findmnt -n -o OPTIONS --target "$ROOT" 2>/dev/null | grep -o 'lowerdir=[^,]*')
         if [ -n "$LOWERDIR" ]; then
            # The first directory in lowerdir is the ephemeral container init layer (unique per container)
            # We must strip it to get the stable image layers.
            # LOWERDIR format: lowerdir=/path/init:/path/image1:/path/image2...
            
            # Remove 'lowerdir=' prefix if captured
            CLEAN_LOWERDIR="${LOWERDIR#lowerdir=}"
            
            # Remove the first path component (everything up to first colon)
            STABLE_LAYERS=$(echo "$CLEAN_LOWERDIR" | sed 's/^[^:]*://')
            
            if [ -n "$STABLE_LAYERS" ] && [ "$STABLE_LAYERS" != "$CLEAN_LOWERDIR" ]; then
               CACHE_SOURCE_STRING="$STABLE_LAYERS"
               [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ] && echo "[DEBUG] Stripped ephemeral layer. Using stable layers: $STABLE_LAYERS" >&2
            else
               # Fallback if only one layer or parsing failed (unlikely for Docker)
               CACHE_SOURCE_STRING="$CLEAN_LOWERDIR"
               [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ] && echo "[DEBUG] Could not strip layer (single layer?), using full: $CLEAN_LOWERDIR" >&2
            fi
         fi
      else 
         # Fallback to parsing mount output (busybox mount compatible)
         # Format varies but usually contains "lowerdir=..." in options
         # Look for mount point at end of line or before " type "
         MOUNT_INFO=$(mount | grep " $ROOT " | grep "overlay")
         LOWERDIR=$(echo "$MOUNT_INFO" | grep -o 'lowerdir=[^,]*' | head -1)
         if [ -n "$LOWERDIR" ]; then
            CACHE_SOURCE_STRING="$LOWERDIR"
            [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ] && echo "[DEBUG] Using lowerdir (from mount) for cache key" >&2
         fi
      fi

      # Use sha256sum if available (coreutils), else standard likely available tools
      RUNCVM_CACHE_KEY=$(echo -n "$CACHE_SOURCE_STRING" | sha256sum 2>/dev/null | awk '{print $1}')
      if [ -z "$RUNCVM_CACHE_KEY" ]; then
         # Fallback if sha256sum missing
         RUNCVM_CACHE_KEY=$(echo -n "$CACHE_SOURCE_STRING" | md5sum 2>/dev/null | awk '{print $1}')
      fi
      
      log "FIRECRACKER: Rootfs Cache Key: $RUNCVM_CACHE_KEY (derived from $ROOT)"
      [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ] && echo "[DEBUG] FIRECRACKER: Cache Key: $RUNCVM_CACHE_KEY" >&2
      
      set_config_env "RUNCVM_CACHE_KEY" "$RUNCVM_CACHE_KEY"
    fi

    # 2. Mount Cache Directory
    HOST_CACHE_DIR="/var/lib/runcvm/cache"
    CTR_CACHE_DIR="/.runcvm/cache"
    
    # Ensure host cache dir exists (might fail if read-only host FS, but usually /var/lib is writable in Colima/VMs)
    mkdir -p "$HOST_CACHE_DIR" 2>/dev/null || true

    # Mount it
    jq_set "$CFG" --arg host "$HOST_CACHE_DIR" --arg ctr "$CTR_CACHE_DIR" \
      '.mounts += [{"destination":$ctr,"type":"bind","source":$host,"options":["bind","private","rw"]}]'

    # Pick random port in range 1000-1050
    NFS_PORT=$((1000 + RANDOM % 51))
    
    # Get UID/GID from container config (defaults to 0:0)
    CONTAINER_UID=$(jq_get "$CFG" '.process.user.uid // 0')
    CONTAINER_GID=$(jq_get "$CFG" '.process.user.gid // 0')
    
    # Build NFS volumes string for environment variable
    NFS_VOLUMES=""
    
    # Use process substitution to avoid subshell (so NFS_PORT persists)
    while IFS=: read -r src dst; do
      [ -z "$src" ] && continue
      [ -z "$dst" ] && continue
      case "$dst" in
        /etc/resolv.conf|/etc/hosts|/etc/hostname) continue ;;
      esac
      case "$src" in
        */.runcvm*) continue ;;
      esac
      if [ -e "$src" ]; then
        log "FIRECRACKER:   Volume: $src -> $dst"
        [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ] && echo "[DEBUG] FIRECRACKER: Volume: $src -> $dst (port $NFS_PORT)" >&2
        [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ] && echo "[DEBUG] FIRECRACKER: Volume: $src -> $dst (port $NFS_PORT)" >&2
        
        # Build env var (pipe-separated for multiple volumes)
        if [ -n "$NFS_VOLUMES" ]; then
          NFS_VOLUMES="${NFS_VOLUMES}|${src}:${dst}:${NFS_PORT}"
        else
          NFS_VOLUMES="${src}:${dst}:${NFS_PORT}"
        fi
        
        # Start NFS daemon on HOST for this volume
        if [ -x /opt/runcvm/scripts/runcvm-nfsd ]; then
          log "FIRECRACKER: Starting HOST unfsd for $src (port $NFS_PORT)..."
          [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ] && echo "[DEBUG] Starting unfsd: $src port=$NFS_PORT uid=$CONTAINER_UID" >&2
          
          # Export RUNCVM_LOG_LEVEL so runcvm-nfsd can use it
          export RUNCVM_LOG_LEVEL
          /opt/runcvm/scripts/runcvm-nfsd start "$ID" "$src" "$dst" "$NFS_PORT" "$CONTAINER_UID" "$CONTAINER_GID" 2>&1
          NFS_PORT=$((NFS_PORT + 2))
        else
          log "WARNING: runcvm-nfsd not found at /opt/runcvm/scripts/runcvm-nfsd"
          [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ] && echo "[DEBUG] ERROR: runcvm-nfsd not found!" >&2
        fi
      fi
    done < <(jq -r --arg vm "$RUNCVM_VM_MOUNTPOINT" '
      .mounts[] | 
      select(.type == "bind") | 
      select(.destination | startswith($vm + "/")) |
      "\(.source):\(.destination | ltrimstr($vm))"
    ' "$CFG" 2>/dev/null)
    
    # Pass NFS volumes via environment variable (bypasses file path issues)
    if [ -n "$NFS_VOLUMES" ]; then
      log "FIRECRACKER: Setting RUNCVM_NFS_VOLUMES=$NFS_VOLUMES"
      [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ] && echo "[DEBUG] FIRECRACKER: Setting RUNCVM_NFS_VOLUMES=$NFS_VOLUMES" >&2
      set_config_env "RUNCVM_NFS_VOLUMES" "$NFS_VOLUMES"
    fi
    
    if [ -z "$NFS_VOLUMES" ]; then
      log "FIRECRACKER: No user volumes found"
      [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ] && echo "[DEBUG] FIRECRACKER: No user volumes found" >&2
    fi
  fi
  # Mount / from container to $RUNCVM_VM_MOUNTPOINT, recursively binding all pre-existing mount points
  # (these being only the ones defined ahead of this item in the mounts[] array - so order matters!)
  jq_set "$CFG" --arg root "$ROOT" --arg vm "$RUNCVM_VM_MOUNTPOINT" '.mounts |= [{"destination":$vm,"type":"bind","source":$root,"options":["rbind","private","rw"]}] + .'

  # Mount /opt/runcvm from host to container
  # Define this at top of mounts[] so it is recursively mounted
  # and before (but after in the mounts[] array) /.runcvm so it can be mounted inside it
  jq_set "$CFG" --arg runcvm "$RUNCVM" --arg runcvm_guest "$RUNCVM_GUEST" '.mounts |= [{"destination":$runcvm_guest,"type":"bind","source":$runcvm,"options":["bind","private","ro"]}] + .'

  # Mount a tmpfs at /.runcvm in container
  # Define this at top of mounts[] so it is recursively mounted
  jq_set "$CFG" '.mounts |= [{"destination":"/.runcvm","type":"tmpfs","source":"runcvm","options":["nosuid","noexec","nodev","size=1M","mode=700"]}] + .'

  # Mount a tmpfs at /run in container
  # Define this at bottom of mounts[] so it is not recursively mounted to /vm
  jq_set "$CFG" '.mounts += [{"destination":"/run","type":"tmpfs","source":"run","options":["nosuid","noexec","nodev","size=1M","mode=700"]}]'

  # Mount the API socket directory
  # Provide write access to container so Firecracker can create the socket
  jq_set "$CFG" --arg host "$RUNCVM_SOCKET_DIR" '.mounts += [{"destination":"/.runcvm/sockets","type":"bind","source":$host,"options":["bind","private","rw"]}]'

  # DETERMINE LAUNCH KERNEL:
  #
  # 1. If RUNCVM_KERNEL specified:
  #    - <dist> or <dist>/latest - use latest RUNCVM kernel available for this dist *and* ARGS
  #    - <dist>/<version> - use specific RUNCVM kernel version for this dist *and* ARGS
  # 2. Else, check /etc/os-release and:
  #    a. Use builtin kernel for this dist (if present in the expected location) *and* ARGS
  #    b. Use latest RUNCVM kernel available for the dist:
  #      - ID=alpine, VERSION_ID=3.16.0 => alpine/latest
  #      - ID=debian, VERSION_ID=11     => debian/latest
  #      - ID=ubuntu, VERSION_ID=22.04  => ubuntu/latest
  
  # Look for RUNCVM_KERNEL env var
  RUNCVM_KERNEL=$(get_config_env 'RUNCVM_KERNEL')
  log "RUNCVM_KERNEL='$RUNCVM_KERNEL' (1)"

  # Generate:
  # - RUNCVM_KERNEL_ID: the distro name (e.g. alpine, debian, ubuntu)
  # - RUNCVM_KERNEL_IDVER: the distro name and kernel version (e.g. alpine/5.15.59-0-virt, debian/5.10.0-16-amd64)

  if [ -n "$RUNCVM_KERNEL" ]; then
    # If found, validate
  
    if [[ "$RUNCVM_KERNEL" =~ \.\. ]]; then
      error "Kernel '$RUNCVM_KERNEL' invalid (contains '..')"
    fi
  
    if ! [[ "$RUNCVM_KERNEL" =~ ^[a-z]+(/[^/]+)?$ ]]; then
      error "Kernel '$RUNCVM_KERNEL' invalid (should match ^[a-z]+(/[^/]+)?$)"
    fi
  
    if ! [ -d "$RUNCVM_KERNELS/$RUNCVM_KERNEL" ]; then
      error "Kernel '$RUNCVM_KERNEL' not found (check $RUNCVM_KERNELS)"
    fi

    # If RUNCVM_KERNEL is a distro name only, append /latest
    if [[ "$RUNCVM_KERNEL" =~ ^[a-z]+$ ]]; then
      RUNCVM_KERNEL_IDVER="$RUNCVM_KERNEL/latest"
    else
      RUNCVM_KERNEL_IDVER="$RUNCVM_KERNEL"
    fi  

    RUNCVM_KERNEL_ID=$(dirname "$RUNCVM_KERNEL_IDVER") # Returns e.g. alpine, debian, ubuntu

  else
    # If not found, look for value from /etc/os-release in the container image
    
    RUNCVM_KERNEL_ID=$(load_env_from_file "$ROOT/etc/os-release" "ID")

    # Currently unused
    # RUNCVM_KERNEL_OS_VERSION_ID=$(load_var_from_env "$ROOT/etc/os-release" "VERSION_ID")

    # If still not found, assign a default
    if [ -z "$RUNCVM_KERNEL_ID" ]; then
      RUNCVM_KERNEL_ID="${RUNCVM_KERNEL_DEFAULT:-debian}"
    fi

    RUNCVM_KERNEL_IDVER="$RUNCVM_KERNEL_ID/latest"
  fi
  
  log "RUNCVM_KERNEL='$RUNCVM_KERNEL' (2)"
  log "RUNCVM_KERNEL_ID='$RUNCVM_KERNEL_ID'"
  log "RUNCVM_KERNEL_IDVER='$RUNCVM_KERNEL_IDVER'"
  
  # Now look up the default kernel and initramfs paths and args for this kernel
  case "$RUNCVM_KERNEL_ID" in
          debian) RUNCVM_KERNEL_OS_KERNEL_PATH="/vmlinuz"
                  RUNCVM_KERNEL_OS_INITRAMFS_PATH="/initrd.img"
                  RUNCVM_KERNEL_ROOT="rootfstype=virtiofs root=runcvmfs noresume net.ifnames=1"
                  ;;
          ubuntu) RUNCVM_KERNEL_OS_KERNEL_PATH="/boot/vmlinuz"
                  RUNCVM_KERNEL_OS_INITRAMFS_PATH="/boot/initrd.img"
                  RUNCVM_KERNEL_ROOT="rootfstype=virtiofs root=runcvmfs noresume net.ifnames=1"
                  ;;
              ol) RUNCVM_KERNEL_OS_KERNEL_PATH="/boot/vmlinuz"
                  RUNCVM_KERNEL_OS_INITRAMFS_PATH="/boot/initramfs"
                  RUNCVM_KERNEL_ROOT="root=virtiofs:runcvmfs noresume net.ifnames=1"
                  ;;
  alpine|openwrt) RUNCVM_KERNEL_OS_KERNEL_PATH="/boot/vmlinuz-virt"
                  RUNCVM_KERNEL_OS_INITRAMFS_PATH="/boot/initramfs-virt"
                  RUNCVM_KERNEL_ROOT="rootfstype=virtiofs root=runcvmfs resume="
                  ;;

           *) error "Unrecognised image O/S '$RUNCVM_KERNEL'; specify --env=RUNCVM_KERNEL=<dist> or --env=RUNCVM_KERNEL=<dist>/<version>"; ;;
  esac
  
  # If no RUNCVM_KERNEL specified, look for a kernel and initramfs at the expected paths in the container image.
  if [[ -z "$RUNCVM_KERNEL" && -f "$ROOT/$RUNCVM_KERNEL_OS_KERNEL_PATH" && -f "$ROOT/$RUNCVM_KERNEL_OS_INITRAMFS_PATH" ]]; then
    RUNCVM_KERNEL_PATH="$RUNCVM_KERNEL_OS_KERNEL_PATH"
    RUNCVM_KERNEL_INITRAMFS_PATH="$RUNCVM_KERNEL_OS_INITRAMFS_PATH"
  else
    # If RUNCVM_KERNEL was specified, or we didn't find a kernel and initramfs at the expected paths in the container image,
    # select the latest RUNCVM kernel version and arrange to mount it.

    # Firecracker hypervisor detected - skip module mounting since
    # Firecracker uses its own kernel with built-in modules (no external .ko files needed)
    log "Firecracker hypervisor detected - skipping kernel module bind-mount"
    # Set dummy values for logging
    RUNCVM_KERNEL_VERSION="firecracker"
    RUNCVM_KERNEL_MODULES_SRC=""
    RUNCVM_KERNEL_MODULES_DST=""
  fi

  log "RUNCVM_KERNEL='$RUNCVM_KERNEL'"
  log "RUNCVM_KERNEL_ID='$RUNCVM_KERNEL_ID'"
  log "RUNCVM_KERNEL_VERSION='$RUNCVM_KERNEL_VERSION'"
  log "RUNCVM_KERNEL_OS_KERNEL_PATH='$RUNCVM_KERNEL_OS_KERNEL_PATH'"
  log "RUNCVM_KERNEL_OS_INITRAMFS_PATH='$RUNCVM_KERNEL_OS_INITRAMFS_PATH'"
  log "RUNCVM_KERNEL_PATH='$RUNCVM_KERNEL_PATH'"
  log "RUNCVM_KERNEL_INITRAMFS_PATH='$RUNCVM_KERNEL_INITRAMFS_PATH'"
  log "RUNCVM_KERNEL_ROOT='$RUNCVM_KERNEL_ROOT'"
  log "RUNCVM_KERNEL_MODULES_SRC='$RUNCVM_KERNEL_MODULES_SRC'"
  log "RUNCVM_KERNEL_MODULES_DST='$RUNCVM_KERNEL_MODULES_DST'"
  
  set_config_env "RUNCVM_KERNEL_PATH" "$RUNCVM_KERNEL_PATH"
  set_config_env "RUNCVM_KERNEL_INITRAMFS_PATH" "$RUNCVM_KERNEL_INITRAMFS_PATH"
  set_config_env "RUNCVM_KERNEL_ROOT" "$RUNCVM_KERNEL_ROOT"

  # Configure devices

  # Allow access to KVM and TUN devices
  jq_set "$CFG" '.linux.resources.devices += [{"allow":true,"type":"c","major":10,"minor":232,"access":"rwm"},{"allow":true,"type":"c","major":10,"minor":200,"access":"rwm"}]'
  jq_set "$CFG" '.linux.devices+=[{"path":"/dev/net/tun","type":"c","major":10,"minor":200,"fileMode":8630,"uid":0,"gid":0},{"path":"/dev/kvm","type":"c","major":10,"minor":232,"fileMode":8630,"uid":0,"gid":0}]'
  
  # For now, hardcode --security-opt=seccomp=unconfined;
  # later, we can work out the minimal seccomp permissions required.
  jq_set "$CFG" '.linux.seccomp |= empty'
  
  # CONFIGURE MEMORY
  # Set /dev/shm to RUNCVM_MEM_SIZE env var, or to default
  # - it should be large enough to support VM memory
  RUNCVM_MEM_LIMIT=$(jq_get "$CFG" '.linux.resources.memory.limit')
  log "RUNCVM_MEM_LIMIT=$RUNCVM_MEM_LIMIT"
  if [ "$RUNCVM_MEM_LIMIT" != "null" ]; then
    RUNCVM_MEM_SIZE="$(( $RUNCVM_MEM_LIMIT/1024/1024 ))"
  else
    RUNCVM_MEM_SIZE="$RUNCVM_MEM_SIZE_DEFAULT"
  fi
  log "RUNCVM_MEM_SIZE=${RUNCVM_MEM_SIZE}M"
  set_config_env "RUNCVM_MEM_SIZE" "${RUNCVM_MEM_SIZE}M"
  
  # Save BOOT memory size to state file for later balloon calculations
  echo "$RUNCVM_MEM_SIZE" > "$RUNCVM_SOCKET_DIR/boot_memory_mb"

  RUNCVM_HUGETLB=$(get_config_env "RUNCVM_HUGETLB")
  if [ "$RUNCVM_HUGETLB" != "1" ]; then
    jq_set "$CFG" --arg size "${RUNCVM_MEM_SIZE}M" '( .mounts[] | select(.destination == "/dev/shm") ) = {"destination": "/dev/shm","type": "tmpfs","source": "shm","options": ["nosuid","noexec","nodev","mode=1777","size=" + $size]}'
  # else
    # --shm-size applies; default 64m.
  fi

  # Set the container memory limit with a reasonable additional reserve to allow for
  # QEMU, virtiofsd, dnsmasq and other container memory footprint.
  RUNCVM_MEM_TOTAL_BYTES="$((($RUNCVM_MEM_SIZE + $RUNCVM_CONTAINER_MEM_OVERHEAD)*1024*1024))"
  jq_set "$CFG" --arg size "$RUNCVM_MEM_TOTAL_BYTES" '.linux.resources.memory.limit = ($size | tonumber)'
  # Also set swap limit to same value to avoid "memory+swap limit >= memory limit" error on cgroup v2
  jq_set "$CFG" --arg size "$RUNCVM_MEM_TOTAL_BYTES" '.linux.resources.memory.swap = ($size | tonumber)'

  # Add non-default capabilities needed by:
  # - Docker: CAP_NET_ADMIN
  # - Podman: CAP_NET_ADMIN, CAP_NET_RAW, CAP_MKNOD, CAP_AUDIT_WRITE
  for field in bounding effective permitted
  do
    jq_set "$CFG" --arg field "bounding" '.process.capabilities[$field] |= (.+ ["CAP_NET_ADMIN","CAP_NET_RAW","CAP_MKNOD","CAP_AUDIT_WRITE"] | unique)'
  done
  
  # Filter for RUNCVM_SYS_ADMIN=1
  RUNCVM_SYS_ADMIN=$(get_config_env "RUNCVM_SYS_ADMIN")
  if [ "$RUNCVM_SYS_ADMIN" = "1" ]; then
    # TODO use 'unique'
    jq_set "$CFG" '.process.capabilities.bounding += ["CAP_SYS_ADMIN"] | .process.capabilities.effective += ["CAP_SYS_ADMIN"] | .process.capabilities.permitted += ["CAP_SYS_ADMIN"]'
  fi

  # Save formatted config.json for debugging
  jq -r . <$CFG >/tmp/config.json-$$-2 2>/dev/null || true
  
elif [ "$COMMAND" = "exec" ]; then

  log_debug "Command: exec"

  # USAGE:
  #   runc exec [command options] <container-id> <command> [command options]  || -p process.json <container-id>
  #
  # PARSE 'exec' COMMAND OPTIONS
  # --console-socket value             path to an AF_UNIX socket which will receive a file descriptor referencing the master end of the console's pseudoterminal
  # --cwd value                        current working directory in the container
  # --env value, -e value              set environment variables
  # --tty, -t                          allocate a pseudo-TTY
  # --user value, -u value             UID (format: <uid>[:<gid>])
  # --additional-gids value, -g value  additional gids
  # --process value, -p value          path to the process.json
  # --detach, -d                       detach from the container's process
  # --pid-file value                   specify the file to write the process id to
  # --process-label value              set the asm process label for the process commonly used with selinux
  # --apparmor value                   set the apparmor profile for the process
  # --no-new-privs                     set the no new privileges value for the process
  # --cap value, -c value              add a capability to the bounding set for the process
  # --preserve-fds value               Pass N additional file descriptors to the container (stdio + $LISTEN_FDS + N in total) (default: 0)
  # --cgroup value                     run the process in an (existing) sub-cgroup(s). Format is [<controller>:]<cgroup>.
  # --ignore-paused                    allow exec in a paused container    
  while true
  do
    case "$1" in
      --console-socket|--cwd|--env|-e|--user|-u|--additional-gids|-g|--pid-file|--process-label|--apparmor|--cap|-c|--preserve-fds|--cgroup) shift; shift; continue; ;;
      --tty|-t|--detach|-d|--no-new-privs|--ignore-paused) shift; continue; ;;
      --process|-p) shift; PROCESS="$1"; continue; ;;
      *) break; ;;
    esac
  done

  # Allow user to enable debug logging
  if [ "$(get_process_env "$PROCESS" 'RUNCVM_RUNTIME_DEBUG' '0')" = "1" ]; then
    RUNCVM_LOG_LEVEL="DEBUG"
    CURRENT_LOG_LEVEL="${LOG_LEVELS[DEBUG]}"
  fi

  log_debug "Command line: $0 ${COMMAND_LINE[@]@Q}"
  log_debug "Command: exec process=$PROCESS"
  
  # Save formatted process.json
  jq -r . <$PROCESS >/tmp/process.json-$$-1

  ARG1=$(jq_get "$PROCESS" '.args[0]')
  if [ "$ARG1" = "---" ]; then
    jq_set "$PROCESS" 'del(.args[0])'
  else
    uidgid=$(jq_get "$PROCESS" '(.user.uid | tostring) + ":" + (.user.gid | tostring) + ":" + ((.user.additionalGids // []) | join(","))')
    cwd=$(jq_get "$PROCESS" '.cwd')
    hasHome=$(get_process_env_boolean "$PROCESS" 'HOME')
    wantsTerminal=$(jq_get "$PROCESS" '.terminal')

    jq_set "$PROCESS" \
      --arg exec "$RUNCVM_EXEC" \
      --arg uidgid "$uidgid" \
      --arg cwd "$cwd" \
      --arg hasHome "$hasHome" \
      --arg wantsTerminal "$wantsTerminal" \
      '.args |= [$exec, $uidgid, $cwd, $hasHome, $wantsTerminal] + .'

    # Force root (or whatever user qemu runs as)
    # Force cwd in the container to / 
    jq_set "$PROCESS" '.user = {"uid":0, "gid":0} | .cwd="/"'
  fi
  
  # Save formatted process.json for debugging
  jq -r . <$PROCESS >/tmp/process.json-$$-2 2>/dev/null || true

elif [ "$COMMAND" = "delete" ]; then

  log_debug "Command: delete"
  
  # USAGE:
  #   runc delete [command options] <container-id>
  #
  # PARSE 'delete' COMMAND OPTIONS
  # --force, -f  forcibly deletes the container if it is still running (uses SIGKILL)
  
  while true
  do
    case "$1" in
      --force|-f) shift; continue; ;;
      *) break; ;;
    esac
  done
  
  ID="$1"
  
  if [ -n "$ID" ]; then
    log "FIRECRACKER: Cleaning up NFS for container $ID"
    echo "[DEBUG] FIRECRACKER: Calling runcvm-nfsd stop-all for $ID" >&2
    
    # Stop unfsd daemon for this container
    if [ -x /opt/runcvm/scripts/runcvm-nfsd ]; then
      /opt/runcvm/scripts/runcvm-nfsd stop-all "$ID" 2>&1 || true
      log "FIRECRACKER: NFS cleanup complete for $ID"
    else
      log "WARNING: runcvm-nfsd not found at /opt/runcvm/scripts/runcvm-nfsd"
    fi
  fi
  
elif [ "$COMMAND" = "update" ]; then

  log_debug "Command: update"
  
  # USAGE:
  #   runc update [command options] <container-id>
  #
  # PARSE 'update' COMMAND OPTIONS
  # ...
  
  # DEBUG: Log all arguments unconditionally if runtime.log exists
  if [ -f "/opt/runcvm/runtime.log" ]; then
    echo "UPDATE CMD: $0 ${COMMAND_LINE[@]}" >> /opt/runcvm/runtime.log
  fi

  # We need to capture memory argument to perform ballooning
  UPDATE_MEMORY_BYTES=""
  
  # Parse args to find memory update
  # We iterate over args but we must NOT shift them away permanently
  ARGS=("${COMMAND_LINE[@]}")
  
  # Iterate starting from index 1 (skipping $0)
  for ((i=1; i<${#ARGS[@]}; i++)); do
      arg="${ARGS[$i]}"
      case "$arg" in
          --memory|-m) 
              # Next arg is the value
              next_idx=$((i+1))
              UPDATE_MEMORY_BYTES="${ARGS[$next_idx]}"
              ;;
          --memory=*)
              UPDATE_MEMORY_BYTES="${arg#*=}"
              ;;
          --resources|-r)
              # Read resources from JSON file
              next_idx=$((i+1))
              RES_FILE="${ARGS[$next_idx]}"
              
              # Handle stdin input ('-')
              # Docker passes resources via stdin using '-'
              if [ "$RES_FILE" = "-" ]; then
                  TMP_RES_FILE="/tmp/runcvm-resources-$$.json"
                  # Capture stdin to file
                  cat > "$TMP_RES_FILE"
                  
                  # Update our local variable to point to the file
                  RES_FILE="$TMP_RES_FILE"
                  
                  # Rewrite COMMAND_LINE to use the file instead of '-'
                  # We need to find the index in COMMAND_LINE array matching next_idx
                  # Note: ARGS is a copy, so indices might match if we didn't shift.
                  # COMMAND_LINE includes $0 (runcvm-runtime) at index 0?
                  # Yes, COMMAND_LINE=("$@"). And in loop we started at 1.
                  # So COMMAND_LINE[$next_idx] should be '-'.
                  # Let's verify and replace.
                  # Warning: COMMAND_LINE indices are 0-based. ARGS logic used copy.
                  
                  # Safer: just iterate and replace first occurrence of '-' after --resources?
                  # Actually, we can just assume the indices line up since ARGS was a direct copy.
                  # COMMAND_LINE is 0-indexed array of arguments.
                  # Original loop: for ((i=1; ... ARGS=$@ (basically)
                  # So ARGS[i] corresponds to COMMAND_LINE[i-1] ?? 
                  # No, COMMAND_LINE=("$@"). $1 is index 0.
                  # So ARGS[1] ($1) is COMMAND_LINE[0].
                  
                  # Let's fix the loop to iterate properly over indices of COMMAND_LINE
                  # or just replace the value in COMMAND_LINE using the same index.
                  # "for ((i=1; i<${#ARGS[@]}; i++))" iterates 1..N.
                  # COMMAND_LINE has indices 0..N-1.
                  # So ARGS[i] == COMMAND_LINE[i-1].
                  
                  CMD_IDX=$(( i )) # Wait, if ARGS=("${COMMAND_LINE[@]}"), it's an exact copy.
                  # So ARGS[0] is COMMAND_LINE[0].
                  
                  # Wait, line 285: COMMAND_LINE=("$@")
                  # line 959: ARGS=("${COMMAND_LINE[@]}")
                  # So indices match exactly.
                  
                  COMMAND_LINE[$next_idx]="$TMP_RES_FILE"
                  log_debug "Rewrote --resources - to $TMP_RES_FILE"
              fi

              if [ -f "$RES_FILE" ]; then
                  # Extract memory.limit from JSON (if present)
                  # Structure: { "memory": { "limit": <bytes>, ... }, ... }
                  # or OCI spec: linux.resources.memory.limit
                  # runc expects a specialized format or OCI? Docker passes OCI LinuxResources struct usually.
                  # Let's inspect the file content in debug log
                  if [ -f "/opt/runcvm/runtime.log" ]; then
                     echo "RESOURCES FILE ($RES_FILE):" >> /opt/runcvm/runtime.log
                     cat "$RES_FILE" >> /opt/runcvm/runtime.log
                  fi
                  
                  # Try to parse limit using jq_get
                  # Note: runc update resources file is usually just the LinuxResources object
                  LIMIT=$(jq_get "$RES_FILE" '.memory.limit')
                  if [ -n "$LIMIT" ] && [ "$LIMIT" != "null" ] && [ "$LIMIT" != "0" ]; then
                      UPDATE_MEMORY_BYTES="$LIMIT"
                  fi
              fi
              ;;
      esac
  done
  
  # Get Container ID (last argument usually)
  
  # Get Container ID (last argument usually)
  ID="${ARGS[${#ARGS[@]}-1]}"
  
  if [ -n "$UPDATE_MEMORY_BYTES" ] && [ -n "$ID" ]; then
      # Calculate balloon size
      # 1. Get Boot Memory Size
      RUNCVM_SOCKET_DIR="/var/lib/runcvm/sockets/$ID"
      BOOT_MEM_FILE="$RUNCVM_SOCKET_DIR/boot_memory_mb"
      
      if [ -f "$BOOT_MEM_FILE" ]; then
          BOOT_MEM_MB=$(cat "$BOOT_MEM_FILE")
          
          # Convert update bytes to MB
          TARGET_MEM_MB=$(( UPDATE_MEMORY_BYTES / 1024 / 1024 ))
          
          # Firecracker Balloon Logic:
          # amount_mib = size of the BALLOON (memory taken AWAY from guest)
          # Balloon Size = Boot Size - Target Size
          
          # If Target > Boot, we cannot add memory. Balloon size = 0?
          # Actually checking docs: "amount_mib": Integer. Target balloon size in MiB.
          
          if [ "$TARGET_MEM_MB" -ge "$BOOT_MEM_MB" ]; then
              # User requested more memory than boot size.
              # Best we can do is deflate balloon completely (return to full boot memory).
              BALLOON_SIZE_MB=0
              log "UPDATE: Memory $TARGET_MEM_MB MB >= Boot $BOOT_MEM_MB MB. Deflating balloon to 0 (Max)."
          else
              BALLOON_SIZE_MB=$(( BOOT_MEM_MB - TARGET_MEM_MB ))
              log "UPDATE: Resizing memory to $TARGET_MEM_MB MB (Boot: $BOOT_MEM_MB MB). Setting balloon to $BALLOON_SIZE_MB MB."
          fi
          
          # Call Firecracker API via curl
          # Socket: $RUNCVM_SOCKET_DIR/firecracker.socket
          # API: PUT /balloon
          # Body: { "amount_mib": <size>, "stats_polling_interval_s": 1 }
          
          SOCKET_PATH="$RUNCVM_SOCKET_DIR/firecracker.socket"
          
          if [ -S "$SOCKET_PATH" ]; then
              log_debug "Calling Firecracker API: PUT /balloon (amount_mib: $BALLOON_SIZE_MB)"
              
              if command -v curl >/dev/null 2>&1; then
                   curl --unix-socket "$SOCKET_PATH" -X PATCH "http://localhost/balloon" \
                        -H "Accept: application/json" \
                        -H "Content-Type: application/json" \
                        -d "{ \"amount_mib\": $BALLOON_SIZE_MB }" \
                        >> /opt/runcvm/runtime.log 2>&1 || log_error "Failed to call Firecracker API"
              else
                   log_error "curl not found, cannot update Firecracker balloon"
              fi
          else
             log "WARNING: Firecracker socket not found at $SOCKET_PATH. Is the VM running with API enabled?"
          fi
      fi
  fi
  
fi

log_debug "--- LOG ENDS ---"

exec /usr/bin/runc "${COMMAND_LINE[@]}"
