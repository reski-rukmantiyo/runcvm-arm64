#!/.runcvm/guest/bin/bash -e

# See https://qemu-project.gitlab.io/qemu/interop/qemu-ga-ref.html

# Load original environment
. /.runcvm/config

# Load defaults and aliases
. $RUNCVM_GUEST/scripts/runcvm-ctr-defaults

env() {
  busybox env "$@"
}

to_bin() {
  # tab, LF, space, ', ", \
  tr "\011\012\040\047\042\134" '\200\201\202\203\204\205'
}

# Expects:
# - To be run as root
# - To be given env vars
# - To be given arguments
#   $1 <uid>:<gid>:<additionalGids>
#   $2 <cwd>
#   $3 <ENV-HOME-boolean>
#   $4 <wantsTerminal-boolean>
#   $(5...) <command> <args>

command="$RUNCVM_GUEST/scripts/runcvm-vm-exec"
uidgid="$1"
cwd="$2"
hasHome="$3"
wantsTerminal="$4"
shift 4

# Parse uidgid and construct args array for the call to $command within the VM:
# $1 <uid>
# $2 <gid>
# $3 <cwd>
# $(4...) <command> <args>

IFS=':' read -r uid gid additionalGids <<< "$uidgid"
args=("$@")

if [ ${#args[@]} -gt 0 ]; then
  args_bin=$(printf '%s\n' "${args[@]}" | to_bin)
fi

# If the HOME env var was not set either in the image, or via docker run, or via docker exec,
# then set HOME to the requested user's default homedir.
#
# - See runcvm-ctr-entrypoint for full details of how/why hasHome is needed and HOME gets set.

if [ "$hasHome" != "1" ]; then
  # Either this script needs to look up uid's HOME or else runcvm-vm-exec does; for now, we do it here.
  HOME=$(getent passwd "$uid" | cut -d':' -f6)
fi

# Cache paths before cleaning
DBCLIENT_BIN="$RUNCVM_GUEST/usr/bin/dbclient"
VSOCK_CONNECT_BIN="$RUNCVM_GUEST/scripts/runcvm-vsock-connect"

# Clean RUNCVM env vars
clean_env

# N.B. Only exported env vars will be returned and sent
mapfile -t env < <(env -u _ -u SHLVL -u PWD)

if [ ${#env[@]} -gt 0 ]; then
  env_bin=$(printf '%s\n' "${env[@]}" | to_bin)
fi

if [ "$wantsTerminal" = "true" ]; then
  opts=(-t)
fi

retries=30 # 15 seconds
delay=0 # Signal that extra time should be allowed for RunCVM VM, its init and its dropbear sshd to start after the above conditions are satisfied

while ! [ -s /.runcvm/dropbear/key ] || ! load_network
do
  if [ $retries -gt 0 ]; then
    retries=$((retries-1))
    delay=1
    sleep 0.5
    continue
  fi

  echo "Error: RunCVM container not yet started" >&2
  exit 1
done

# If startup was detected, wait a few extra seconds for dropbear sshd to be ready
if [ "$delay" -ne 0 ]; then
  sleep 2
fi

# Determine the SSH target IP
# Strategy: Loopback Connection (Force 127.0.0.1)
# We now configure the container environment (via runcvm-ctr-firecracker) to
# DNAT 127.0.0.1:22222 to the VM's internal IP OR proxy via VSOCK.
SSH_CONNECT_IP="127.0.0.1"
SSH_TARGET_IP="$SSH_CONNECT_IP" # For dbclient

# Determine IP to check for connectivity (The real VM IP)
# load_network (called in loop above) should have set DOCKER_IF_IP
# If not (e.g. loaded 'default' but it was weird), try to find a valid IP
VM_CHECK_IP="$DOCKER_IF_IP"

if [ -z "$VM_CHECK_IP" ] || [ "$VM_CHECK_IP" = "-" ]; then
  # Fallback: check eth0 explicitly
  if [ -s /.runcvm/network/devices/eth0 ]; then
     read _ _ _ VM_CHECK_IP _ _ < /.runcvm/network/devices/eth0
  fi
fi

# Verify connectivity before connecting
# This avoids "Host is unreachable" errors if the VM isn't fully ready or ARP is converging
# VSOCK CHECK vs IP CHECK
if [ -e "/run/firecracker.vsock" ] && [ -x "$VSOCK_CONNECT_BIN" ]; then
   # VSOCK MODE
   USE_VSOCK=1
   SSH_CONNECT_IP="127.0.0.1" # Connection to local socat proxy
   SSH_TARGET_IP="$SSH_CONNECT_IP"

   log_debug "Using VSOCK transport for exec..."
   # Wait for Socket check (simple connectivity)??
   # Actually, we can just rely on the fallback or retry loop.
   # We don't have an easy way to "ping" VSOCK without connecting.
   # But since we have a retry loop for startup, it might suffice.
else
   # TCP MODE (Legacy / Fallback)
   USE_VSOCK=0
   if [ -n "$VM_CHECK_IP" ] && [ "$VM_CHECK_IP" != "-" ]; then
      log_debug "Waiting for VM ($VM_CHECK_IP)..."
      
      # Wait for VM to answer ping (up to 5 seconds)
      # We need the VM to be reachable via the bridge for socat to work
      check_count=0
      while ! busybox ping -c 1 -W 1 "$VM_CHECK_IP" >/dev/null 2>&1 && [ $check_count -lt 25 ]; do
        sleep 0.2
        check_count=$((check_count+1))
      done
      
      # Wait for SSH Port (22222) to be open (up to 10 seconds)
      # This avoids "Connection refused" if network is up but Dropbear isn't ready
      port_count=0
      while ! nc -z -w 1 "$VM_CHECK_IP" 22222 >/dev/null 2>&1 && [ $port_count -lt 20 ]; do
         sleep 0.5
         port_count=$((port_count+1))
      done
   
      if [ $check_count -ge 25 ]; then
        echo "Warning: VM $VM_CHECK_IP did not respond to ping, connection may fail" >&2
      fi
   else
      # If we can't find the IP, we just proceed and hope sending to localhost works (maybe legacy mode)
      :
   fi
fi

# Debug client key
# Verify client key permissions
# Fixed: LOG_LEVELS array removed, use numeric comparison directly or CURRENT_LOG_LEVEL
# DEBUG is 0
if [ "${CURRENT_LOG_LEVEL:-999}" -le 0 ]; then
  log_debug "Client Key Check:"
  ls -la /.runcvm/dropbear/key >&2
fi

# Finally, execute the command via SSH
# We use dbclient (Dropbear's SSH client)
# -y -y: Always accept hostkey AND suppress the confirmation message
# -i: Identify file
# -p: Port
# Note: Host variable is passed as argument, but we proxy via 127.0.0.1
# using socat. So we connect to root@127.0.0.1
# Start the Proxy (if not already running, but we assume we need one per exec session or rely on global?)
# Actually, the original design had a global socat in runcvm-ctr-firecracker for TCP.
# But for VSOCK, we might want to start one just for this session or check if global is capable.
# The global socat in runcvm-ctr-firecracker is for TCP forwarding.
# If we are in VSOCK mode, we need a local proxy for this session.

if [ "$USE_VSOCK" = "1" ]; then
   # Start a temporary socat proxy for this exec session (on random high port or 22222 collision?)
   # We can't bind to 22222 if the global one is there.
   # But wait, runcvm-ctr-firecracker-k8s starts socat on 127.0.0.1:22222 -> VM_IP:22222.
   # If we use VSOCK, we want 127.0.0.1:XXXX -> VSOCK.
   
   # Let's pick a random port for this session
   PROXY_PORT=$(( 30000 + RANDOM % 10000 ))
   
   # Start socat proxy: TCP-LISTEN -> EXEC:runcvm-vsock-connect
   # Detect socat
   SOCAT_BIN="socat"
   if ! command -v socat >/dev/null 2>&1; then
      if [ -x "/.runcvm/guest/usr/bin/socat" ]; then
         SOCAT_BIN="/.runcvm/guest/usr/bin/socat"
      else
         echo "Error: socat not found for VSOCK transport" >&2
         exit 1
      fi
   fi

   # Start socat proxy: TCP-LISTEN -> EXEC:runcvm-vsock-connect
   # We remove 2>/dev/null to see errors in runcvm.debug or kubectl logs
   $SOCAT_BIN TCP-LISTEN:$PROXY_PORT,bind=127.0.0.1,fork,reuseaddr EXEC:"$VSOCK_CONNECT_BIN /run/firecracker.vsock 22" &
   PROXY_PID=$!
   
   # Cleanup proxy on exit
   trap 'kill $PROXY_PID 2>/dev/null' EXIT
   
   # Wait a bit for socat to start
   sleep 0.1
   
   # Connect dbclient to this proxy port
   exec "$DBCLIENT_BIN" "${opts[@]}" -p $PROXY_PORT -y -y -i /.runcvm/dropbear/key root@127.0.0.1 "$command '$uidgid' '$(echo -n $cwd | to_bin)' '$args_bin' '$env_bin'"

else
   # Original TCP Mode (Global Proxy on 22222)
   exec "$DBCLIENT_BIN" "${opts[@]}" -p 22222 -y -y -i /.runcvm/dropbear/key root@$SSH_TARGET_IP "$command '$uidgid' '$(echo -n $cwd | to_bin)' '$args_bin' '$env_bin'"
fi
