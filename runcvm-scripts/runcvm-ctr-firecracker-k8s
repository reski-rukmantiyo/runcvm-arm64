#!/.runcvm/guest/bin/bash

# RunCVM Firecracker Launcher
# Launches a Firecracker microVM using config file mode (for console output)
#
# STRATEGY: Copy container files to /dev/shm, then create rootfs on overlay
# This avoids the self-copy problem where mke2fs would copy the rootfs into itself

set -o errexit -o pipefail
# set -x # Debug tracing disabled for production speed

# Load original environment
# NOTE: Keep RUNCVM_LOG_LEVEL from pod environment for debugging
# unset RUNCVM_LOG_LEVEL  # DISABLED: We want pod env to propagate
. /.runcvm/config

# Load defaults
. $RUNCVM_GUEST/scripts/runcvm-ctr-defaults && unset PATH

FIRECRACKER_BIN="$RUNCVM_GUEST/sbin/firecracker"
FIRECRACKER_CONFIG="/run/.firecracker-config.json"
NFS_CONFIG="/.runcvm/nfs-mounts"
NFS_PORT_MIN=1000
NFS_PORT_MAX=1050

# ============================================================
# LOGGING SYSTEM
# Severity levels: DEBUG, INFO, ERROR, OFF
# Control via: RUNCVM_LOG_LEVEL environment variable
# ============================================================

# Configure Logger for Firecracker (Container side)
RUNCVM_COMPONENT_NAME="RunCVM-FC"
RUNCVM_LOG_FILE=""   # Disable file logging
RUNCVM_LOG_STDERR=1  # Enable stderr logging

# Source common logging
. "$RUNCVM_GUEST/scripts/common/logging.sh"

# Source Firecracker modules
. "$RUNCVM_GUEST/scripts/docker/firecracker-disk.sh"
. "$RUNCVM_GUEST/scripts/docker/firecracker-network.sh"
. "$RUNCVM_GUEST/scripts/docker/firecracker-config.sh"
. "$RUNCVM_GUEST/scripts/docker/firecracker-init.sh"

# EARLY DEBUG
log "DEBUG: Global scope /.runcvm check:"
ls -la /.runcvm | while read line; do log "  $line"; done
if [ -f /.runcvm/entrypoint ]; then
    log "DEBUG: /.runcvm/entrypoint exists globally"
else
    log "DEBUG: /.runcvm/entrypoint MISSING globally"
fi


# Main function
main() {
  export RUNCVM_LOG_LEVEL=DEBUG
  
  local CMD_ARGS="$*"
  # If no args, default to sleep to keep VM alive for debug
  if [ -z "$CMD_ARGS" ]; then
    CMD_ARGS="/bin/sleep 1000"
  fi
  
  log "Starting Firecracker microVM launcher... [VERSION: DEBUG-FORCE-V5]"
  log "Container command: $CMD_ARGS"
  # ==========================================================================
  # STEP 0: Setup NFS volumes BEFORE creating rootfs
  # ==========================================================================
  


  log "DEBUG: Script Arguments (\$@): $@"
  
  setup_nfs_volumes
  
  log "DEBUG: Content of /.runcvm on host:"
  ls -la /.runcvm | while read line; do log "  $line"; done
  
  if [ -f /.runcvm/config ]; then
      log "DEBUG: Content of /.runcvm/config:"
      busybox cat /.runcvm/config | while read line; do log "  $line"; done
  fi

  log_debug "NFS_CONFIG=$NFS_CONFIG"
  if [ -f "$NFS_CONFIG" ]; then
    log_debug "Contents of $NFS_CONFIG:"
    busybox cat "$NFS_CONFIG" 2>&1 | while read line; do log_debug "  $line"; done
  else
    log_debug "NFS_CONFIG file does not exist (no volumes)"
  fi
  
  # Check for Firecracker binary
  if [ ! -x "$FIRECRACKER_BIN" ]; then
    error "Firecracker binary not found: $FIRECRACKER_BIN"
  fi
  
  # ==========================================================================
  # SSH KEY GENERATION (BEFORE ROOTFS STAGING)
  # ==========================================================================
  # Generate dropbear keys BEFORE rootfs creation so KEY_PUBLIC can be injected
  # into authorized_keys during staging. This is critical for docker exec.
  
  DROPBEAR_DIR="/.runcvm/dropbear"
  busybox mkdir -p "$DROPBEAR_DIR"
  
  # Use explicit path for dynamic linker (RUNCVM_LD may not be set yet)
  local RUNCVM_LD_PATH="$RUNCVM_GUEST/lib/ld"
  
  if [ ! -f "$DROPBEAR_DIR/key" ]; then
    log "Generating SSH keys for docker exec..."
    "$RUNCVM_LD_PATH" "$RUNCVM_GUEST/usr/bin/dropbearkey" -t ed25519 -f "$DROPBEAR_DIR/key" >/dev/null 2>&1
  fi
  
  # Extract public key (MUST happen before rootfs staging)
  KEY_PUBLIC=$("$RUNCVM_LD_PATH" "$RUNCVM_GUEST/usr/bin/dropbearkey" -y -f "$DROPBEAR_DIR/key" 2>/dev/null | grep ^ssh)
  
  if [ -n "$KEY_PUBLIC" ]; then
    log "SSH public key extracted successfully"
    log_debug "KEY_PUBLIC=$KEY_PUBLIC"
  else
    log_error "Failed to extract SSH public key - docker exec will not work"
  fi
  
  # ==========================================================================
  # CACHE MAINTENANCE (LRU Eviction)
  # ==========================================================================
  # Clean up old cache files to prevent disk fill-up
  # Delete files accessed more than 24 hours ago (-atime +1) or modified (-mtime +1)
  if [ -d "/.runcvm/cache" ]; then
    log_debug "Running cache maintenance..."
    # Run in background to not block boot
    (
      find /.runcvm/cache -name "*.ext4" -type f -mtime +1 -delete 2>/dev/null || true
    ) &
  fi

  # ==========================================================================
  # STRATEGY: Rootfs Creation with Caching
  # ==========================================================================
  
  local rootfs_path="/rootfs.ext4"
  local entrypoint_file="/.runcvm/entrypoint"
  local staging_dir="/tmp/rootfs-staging"
  # Append version to invalidate old caches with stale init scripts
  local cache_key="${RUNCVM_CACHE_KEY}-v7"
  local cache_file=""
  local use_cache=0

  # Check if caching is globally enabled via env var (Default: 1/true)
  # USER INSTRUCTION: DO NOT ENABLE RUNCVM CACHE
  local cache_active="0"
  
  if [ "$cache_active" = "1" ] || [ "$cache_active" = "true" ]; then
      if [ -n "$cache_key" ] && [ -d "/.runcvm/cache" ]; then
        cache_file="/.runcvm/cache/${cache_key}.ext4"
        use_cache=1
        log "Cache Key: $cache_key"
      fi
  else
      log "Rootfs caching DISABLED via ROOTFS_CACHE_ACTIVE=$cache_active"
  fi

  # CACHE HIT CHECK
  if [ "$use_cache" = "1" ] && [ -f "$cache_file" ]; then
    log "Cache HIT! Using cached rootfs: $cache_file"
    busybox mkdir -p "$staging_dir"
    
    # Create init script (always fresh)
    generate_init_script "${staging_dir}/init"

    # Generate network configs for injection
    generate_network_configs "$staging_dir"
    
    # Fast copy small image
    if cp --sparse=always "$cache_file" "$rootfs_path" 2>/dev/null; then
       :
    else
       busybox cp "$cache_file" "$rootfs_path"
    fi
    
    # INJECT CURRENT ENTRYPOINT
    # The cached rootfs has a stale entrypoint. We must overwrite it with the current one.
    if command -v debugfs >/dev/null 2>&1 && [ -f "$entrypoint_file" ]; then
       # Run debugfs to update entrypoint (surpress output unless error)
       debugfs -w -R "rm /.runcvm/entrypoint" "$rootfs_path" >/dev/null 2>&1 || true
       if ! debugfs -w -R "write $entrypoint_file /.runcvm/entrypoint" "$rootfs_path" >/dev/null 2>&1; then
          log "  [WARNING] Failed to update entrypoint in rootfs"
       fi
    fi
    
    # INJECT FRESH PRIVATE KEY (CRITICAL so init script derives correct pubkey)
    # The cached rootfs has a stale private key. If we don't update it, the init script
    # will generate a stale public key and overwrite our authorized_keys work.
    if command -v debugfs >/dev/null 2>&1 && [ -f "$DROPBEAR_DIR/key" ]; then
       # Directory structure should exist in cache, but ensure it does
       debugfs -w -R "mkdir /.runcvm" "$rootfs_path" >/dev/null 2>&1 || true
       debugfs -w -R "mkdir /.runcvm/dropbear" "$rootfs_path" >/dev/null 2>&1 || true
       
       debugfs -w -R "rm /.runcvm/dropbear/key" "$rootfs_path" >/dev/null 2>&1 || true
       if ! debugfs -w -R "write $DROPBEAR_DIR/key /.runcvm/dropbear/key" "$rootfs_path" >/dev/null 2>&1; then
          log "  [WARNING] Failed to inject dropbear private key into cached rootfs"
       else
          log "  Injected fresh dropbear private key into cached rootfs"
       fi
    fi
    
    # INJECT CURRENT HOSTNAME
    # Cached rootfs has stale /etc/hostname. Update it.
    if command -v debugfs >/dev/null 2>&1 && [ -f /etc/hostname ]; then
       local hostname_tmp="/tmp/hostname.$$"
       busybox cp /etc/hostname "$hostname_tmp"
       debugfs -w -R "rm /etc/hostname" "$rootfs_path" >/dev/null 2>&1 || true
       if ! debugfs -w -R "write $hostname_tmp /etc/hostname" "$rootfs_path" >/dev/null 2>&1; then
          log "  [WARNING] Failed to inject hostname into cached rootfs"
       else
          log "  Injected fresh hostname into cached rootfs"
       fi
       busybox rm -f "$hostname_tmp"
    fi
    
    # INJECT CURRENT AUTHORIZED_KEYS (CRITICAL for docker exec)
    # The cached rootfs has stale authorized_keys. We must inject the fresh public key.
    if command -v debugfs >/dev/null 2>&1 && [ -n "$KEY_PUBLIC" ]; then
       # Create temp file with current public key
       local auth_keys_tmp="/tmp/authorized_keys.$$"
       echo "$KEY_PUBLIC" > "$auth_keys_tmp"
       
       # Ensure /root/.ssh directory exists in rootfs
       debugfs -w -R "mkdir /root/.ssh" "$rootfs_path" >/dev/null 2>&1 || true
       
       # Remove old authorized_keys and inject new one
       debugfs -w -R "rm /root/.ssh/authorized_keys" "$rootfs_path" >/dev/null 2>&1 || true
       if ! debugfs -w -R "write $auth_keys_tmp /root/.ssh/authorized_keys" "$rootfs_path" >/dev/null 2>&1; then
          log "  [WARNING] Failed to inject authorized_keys into cached rootfs"
       else
          log "  Injected fresh authorized_keys into cached rootfs"
       fi
       
       busybox rm -f "$auth_keys_tmp"
       busybox rm -f "$auth_keys_tmp"
    fi
    
    # INJECT CONSOLE FILE (CRITICAL for init script)
    # The cached rootfs might be missing /.runcvm/console. Inject it.
    if command -v debugfs >/dev/null 2>&1; then
       local console_tmp="/tmp/console.$$"
       echo "ttyS0" > "$console_tmp"
       
       debugfs -w -R "mkdir /.runcvm" "$rootfs_path" >/dev/null 2>&1 || true
       debugfs -w -R "rm /.runcvm/console" "$rootfs_path" >/dev/null 2>&1 || true
       if ! debugfs -w -R "write $console_tmp /.runcvm/console" "$rootfs_path" >/dev/null 2>&1; then
          log "  [WARNING] Failed to inject console file into cached rootfs"
       else
          log "  Injected console file into cached rootfs"
       fi
       busybox rm -f "$console_tmp"
       busybox rm -f "$console_tmp"
    fi
    
    # INJECT CONFIG, PWD (CRITICAL for s6-applyuidgid and cd)
    if command -v debugfs >/dev/null 2>&1; then
       # Ensure directory exists first
       debugfs -w -R "mkdir /.runcvm" "$rootfs_path" >/dev/null 2>&1 || true

       # CONFIG
       if [ -f "/.runcvm/config" ]; then
          debugfs -w -R "rm /.runcvm/config" "$rootfs_path" >/dev/null 2>&1 || true
          if ! debugfs -w -R "write /.runcvm/config /.runcvm/config" "$rootfs_path" >/dev/null 2>&1; then
             log "  [WARNING] Failed to inject /.runcvm/config into cached rootfs"
          else
             log "  Injected /.runcvm/config"
          fi
       fi
       
       # PWD
       if [ -f "/.runcvm/pwd" ]; then
          debugfs -w -R "rm /.runcvm/pwd" "$rootfs_path" >/dev/null 2>&1 || true
          if ! debugfs -w -R "write /.runcvm/pwd /.runcvm/pwd" "$rootfs_path" >/dev/null 2>&1; then
             log "  [WARNING] Failed to inject /.runcvm/pwd into cached rootfs"
          else
             log "  Injected /.runcvm/pwd"
          fi
       fi
       
       # ENTRYPOINT (Also inject as /.runcvm/entrypoint just in case, though we did .runcvm-entrypoint earlier?)
       # Wait, earlier code injected `entrypoint_file` to `/.runcvm-entrypoint` (lines 168-174).
       # But `runcvm-vm-start` reads `/.runcvm/entrypoint` (line 23).
       # WE MUST INJECT TO /.runcvm/entrypoint TOO!
       if [ -f "/.runcvm/entrypoint" ]; then
          debugfs -w -R "rm /.runcvm/entrypoint" "$rootfs_path" >/dev/null 2>&1 || true
          if ! debugfs -w -R "write /.runcvm/entrypoint /.runcvm/entrypoint" "$rootfs_path" >/dev/null 2>&1; then
             log "  [WARNING] Failed to inject /.runcvm/entrypoint into cached rootfs"
          else
             log "  Injected /.runcvm/entrypoint"
          fi
       fi
    fi
    
    # Expand to full size
    # Allow user to override via env
    TARGET_SIZE="${RUNCVM_ROOTFS_SIZE:-256M}"
    
    log "Resizing rootfs to $TARGET_SIZE..."
    if command -v truncate >/dev/null 2>&1; then
       truncate -s "$TARGET_SIZE" "$rootfs_path"
    else
       # Fallback to dd (2GB = 2147483648 bytes)
       # Busybox dd might not support G suffix, so use bytes
       local size_bytes=$(( 2 * 1024 * 1024 * 1024 ))
       busybox dd of="$rootfs_path" bs=1 count=0 seek="$size_bytes" 2>/dev/null || true
    fi
    if command -v resize2fs >/dev/null 2>&1; then
       resize2fs "$rootfs_path" >/dev/null 2>&1
    else
       log "WARNING: resize2fs not found, rootfs will be small!"
    fi
    
    log "Rootfs ready (from cache, resized)."
    
  else
    if [ "$use_cache" = "1" ]; then
       log "Cache MISS. Creating new rootfs..."
    else
       log "Cache disabled or unavailable. Creating rootfs from scratch..."
    fi

    # EXISTING LOGIC: Copy container files to /dev/shm, then create rootfs on overlay
    # This avoids the self-copy problem where mke2fs would copy the rootfs into itself
    
    # Use /tmp for staging instead of /dev/shm to avoid ENOSPC on large images
    # staging_dir defined above
    
    # Calculate source size (excluding .runcvm which has RunCVM binaries)
    log "Calculating container size (excluding .runcvm)..."
    
    local source_size=$(busybox du -sm --exclude='.runcvm' "$RUNCVM_VM_MOUNTPOINT" 2>/dev/null | busybox cut -f1)
    [ -z "$source_size" ] && source_size=100
    
    log "Container filesystem size: ${source_size}MB"
    
    # Check if /dev/shm has enough space for staging
    local shm_avail=$(busybox df -m /dev/shm 2>/dev/null | busybox awk 'NR==2 {print $4}')
    [ -z "$shm_avail" ] && shm_avail=0
    local shm_needed=$(( source_size + 50 ))  # Add 50MB buffer
    
    log "/dev/shm available: ${shm_avail}MB, need for staging: ${shm_needed}MB"
    
    if [ "$shm_avail" -eq 0 ]; then
      log "WARNING: Could not determine /dev/shm space (got 0). Proceeding anyway..."
    elif [ "$shm_avail" -lt "$shm_needed" ]; then
      log ""
      log "============================================================"
      log "ERROR: /dev/shm too small for staging"
      log "============================================================"
      log ""
      log "Container size: ${source_size}MB"
      log "/dev/shm available: ${shm_avail}MB"
      log "Needed for staging: ${shm_needed}MB"
      log "/dev/shm is sized by the -m (memory) flag"
      log ""
       log "SOLUTION: Increase VM memory to at least ${shm_needed}m"
       log ""
       log "  docker run --runtime=runcvm -m ${shm_needed}m \\"
       log "    -e RUNCVM_HYPERVISOR=firecracker nginx"
       log ""
       log "============================================================"
       # Fatal error - exit
       exit 1
    fi
    
    # Step 1: Copy container files to staging area
    log "Step 1/3: Copying container files to staging area..."
    log "  Source: $RUNCVM_VM_MOUNTPOINT (excluding .runcvm)"
    log "  Destination: $staging_dir"
    log "  This may take 30-60 seconds for large images..."
    
    busybox rm -rf "$staging_dir"
    busybox mkdir -p "$staging_dir"
    
    # Use tar to copy, excluding .runcvm directory
    local copy_start=$(busybox date +%s)
    
    if ! (cd "$RUNCVM_VM_MOUNTPOINT" && busybox tar -cf - --exclude='.runcvm' . 2>/dev/null | busybox tar -xf - -C "$staging_dir" 2>/dev/null); then
      error "Failed to copy container files to staging"
    fi
    
    local copy_end=$(busybox date +%s)
    local copy_time=$(( copy_end - copy_start ))
    
    local staged_size=$(busybox du -sm "$staging_dir" 2>/dev/null | busybox cut -f1)
    log "  Staging complete: ${staged_size}MB copied in ${copy_time}s"



   # Step 1b: Create init script in staging area
   log "  Creating init script..."
   log " # Export log level to init script"
   
   # CRITICAL: Export RUNCVM_LOG_LEVEL so it gets embedded in generated init
   # The env var from pod must be available when generate_init_script() runs
   export RUNCVM_LOG_LEVEL="${RUNCVM_LOG_LEVEL:-DEBUG}"
   
   local init_script="${staging_dir}/init"
   generate_init_script "$init_script"


   # K8s uses same generated init as Docker - network from /.runcvm-network-eth0
  
  
  # Save the original entrypoint if we have it
  if [ -f "$entrypoint_file" ]; then
    busybox mkdir -p "${staging_dir}/.runcvm"
    busybox cp "$entrypoint_file" "${staging_dir}/.runcvm/entrypoint"
    log "  Saved entrypoint: $(busybox head -1 ${staging_dir}/.runcvm/entrypoint)"
  fi
  
  # Copy essential networking tools from RunCVM guest
  # These are needed because minimal images (like nginx) don't have ip/ifconfig
  # IMPORTANT: We must copy to the SAME path (/.runcvm/guest) because the binaries
  # use relative RPATH that depends on the directory structure
  log "  Copying network tools to rootfs..."
  
  if [ -d "$RUNCVM_GUEST" ]; then
    # Create the same directory structure
    busybox mkdir -p "${staging_dir}/.runcvm/guest"
    
    # Copy the dynamic linker and libraries
    if [ -d "$RUNCVM_GUEST/lib" ]; then
      busybox cp -a "$RUNCVM_GUEST/lib" "${staging_dir}/.runcvm/guest/"
      log "    Copied lib/ (dynamic linker)"
    fi
    
    # Copy usr/lib for additional libraries
    if [ -d "$RUNCVM_GUEST/usr/lib" ]; then
      busybox mkdir -p "${staging_dir}/.runcvm/guest/usr"
      busybox cp -a "$RUNCVM_GUEST/usr/lib" "${staging_dir}/.runcvm/guest/usr/"
      log "    Copied usr/lib/"
    fi
    
    # Copy bin directory with busybox and symlinks
    if [ -d "$RUNCVM_GUEST/bin" ]; then
      busybox cp -a "$RUNCVM_GUEST/bin" "${staging_dir}/.runcvm/guest/"
      log "    Copied bin/ (busybox, ip, etc.)"
    fi
    
    # Copy sbin if exists
    if [ -d "$RUNCVM_GUEST/sbin" ]; then
      busybox cp -a "$RUNCVM_GUEST/sbin" "${staging_dir}/.runcvm/guest/"
      log "    Copied sbin/"
    fi
    
    # Copy usr/bin and usr/sbin
    if [ -d "$RUNCVM_GUEST/usr/bin" ]; then
      busybox mkdir -p "${staging_dir}/.runcvm/guest/usr"
      busybox cp -a "$RUNCVM_GUEST/usr/bin" "${staging_dir}/.runcvm/guest/usr/"
      log "    Copied usr/bin/"
    fi
    if [ -d "$RUNCVM_GUEST/usr/sbin" ]; then
      busybox mkdir -p "${staging_dir}/.runcvm/guest/usr"
      busybox cp -a "$RUNCVM_GUEST/usr/sbin" "${staging_dir}/.runcvm/guest/usr/"
      log "    Copied usr/sbin/"
    fi
    
    # Copy usr/share (contains terminfo for ncurses programs like watch)
    [ -d "$RUNCVM_GUEST/usr/share" ] && busybox mkdir -p "${staging_dir}/.runcvm/guest/usr" && busybox cp -a "$RUNCVM_GUEST/usr/share" "${staging_dir}/.runcvm/guest/usr/" && log "    Copied usr/share/ (terminfo)"
    
    # CRITICAL: mount.nfs and other RPC tools need these in /etc
    busybox mkdir -p "$staging_dir/etc"
    [ -f "$RUNCVM_GUEST/etc/netconfig" ] && busybox cp -p "$RUNCVM_GUEST/etc/netconfig" "$staging_dir/etc/" && log "    Copied /etc/netconfig"
    [ -f "$RUNCVM_GUEST/etc/services" ] && busybox cp -p "$RUNCVM_GUEST/etc/services" "$staging_dir/etc/" && log "    Copied /etc/services"
    
    # Copy tmp/dropbear (contains EPKA library for SSH authentication)
    if [ -d "$RUNCVM_GUEST/tmp/dropbear" ]; then
      busybox mkdir -p "${staging_dir}/.runcvm/guest/tmp"
      busybox cp -a "$RUNCVM_GUEST/tmp/dropbear" "${staging_dir}/.runcvm/guest/tmp/"
      log "    Copied tmp/dropbear/ (EPKA library)"
    fi
    
    # Copy scripts directory (needed for runcvm-vm-exec)
    if [ -d "$RUNCVM_GUEST/scripts" ]; then
      busybox cp -a "$RUNCVM_GUEST/scripts" "${staging_dir}/.runcvm/guest/"
      log "    Copied scripts/"
    fi
  else
    log "  WARNING: RUNCVM_GUEST not found, network tools may not work"
  fi
  
  # Install procps watch wrapper (overrides busybox watch)
  # This provides proper Ctrl-C handling
  # The bundled watch uses BUNDELF and needs the dynamic linker at /.runcvm/guest/lib/ld
  log "  Checking for static watch binary..."
  
  WATCH_BIN=""
  for path in "/.runcvm/guest/bin/watch" "/.runcvm/bin/watch" "/opt/runcvm/bin/watch"; do
    if [ -f "$path" ]; then
      WATCH_BIN="$path"
      break
    fi
  done
  
  if [ -n "$WATCH_BIN" ]; then
    # Remove busybox symlink first (it's at /bin/watch)
    busybox rm -f "${staging_dir}/bin/watch" 2>/dev/null || true
    busybox rm -f "${staging_dir}/usr/bin/watch" 2>/dev/null || true
    
    # Create wrapper script that uses BUNDELF dynamic linker and terminfo
    busybox mkdir -p "${staging_dir}/bin"
    busybox cat > "${staging_dir}/bin/watch" << 'WATCHEOF'
#!/bin/sh
# Wrapper for procps watch (uses BUNDELF dynamic linker)
# Provides proper Ctrl-C handling unlike busybox watch
export TERMINFO=/.runcvm/guest/usr/share/terminfo
exec /.runcvm/guest/lib/ld /.runcvm/guest/bin/watch "$@"
WATCHEOF
    busybox chmod +x "${staging_dir}/bin/watch"
    log "    ✓ Installed procps watch wrapper to /bin/watch"
  else
    log "    ✗ Bundled watch not found"
  fi
  
  # Copy pre-generated dropbear keys from container (generated by runcvm-ctr-entrypoint)
  if [ -d "/.runcvm/dropbear" ] && [ -f "/.runcvm/dropbear/key" ]; then
    busybox mkdir -p "${staging_dir}/.runcvm/dropbear"
    busybox cp -a "/.runcvm/dropbear/"* "${staging_dir}/.runcvm/dropbear/"
    log "    Copied pre-generated dropbear keys"
    
    # INJECT AUTHORIZED_KEYS
    # We must ensure the VM allows login with the key we generated
    if [ -n "$KEY_PUBLIC" ]; then
       busybox mkdir -p "${staging_dir}/root/.ssh"
       echo "$KEY_PUBLIC" > "${staging_dir}/root/.ssh/authorized_keys"
       busybox chmod 700 "${staging_dir}/root/.ssh"
       busybox chmod 600 "${staging_dir}/root/.ssh/authorized_keys"
       log "    Injected /root/.ssh/authorized_keys"
    else
       log "    WARNING: KEY_PUBLIC is empty, cannot inject authorized_keys"
    fi
  fi
  
  # Copy the config file (needed by some scripts)
  if [ -f "/.runcvm/config" ]; then
    busybox cp "/.runcvm/config" "${staging_dir}/.runcvm/config"
    busybox cp "/.runcvm/config" "${staging_dir}/.runcvm/config"
    log "    Copied /.runcvm/config"
  fi
  # Copy nfs-mounts file - CRITICAL for NFS volume mounting in VM
  log_debug "Checking NFS_CONFIG at $NFS_CONFIG"
  if [ -f "$NFS_CONFIG" ]; then
    log_debug "NFS_CONFIG exists, copying to staging..."
    if [ -s "$NFS_CONFIG" ]; then
      busybox cp "$NFS_CONFIG" "${staging_dir}/.runcvm/nfs-mounts"
      log_debug "Copied to ${staging_dir}/.runcvm/nfs-mounts"
      log_debug "Contents:"
      busybox cat "${staging_dir}/.runcvm/nfs-mounts" 2>&1 | while read line; do log_debug "  $line"; done
    else
      log "WARNING: NFS_CONFIG exists but is empty"
    fi
  else
    log "WARNING: NFS_CONFIG does not exist at $NFS_CONFIG"
  fi

  # Copy kernel modules for 9P support from Alpine kernel

  log "  Copying kernel modules..."
  local fc_modules_dir="$RUNCVM_GUEST/kernels/firecracker/latest/modules"
  if [ -d "$fc_modules_dir" ]; then
    # Find the kernel version directory
    local kernel_ver=$(busybox ls "$fc_modules_dir" 2>/dev/null | busybox head -1)
    if [ -n "$kernel_ver" ] && [ -d "$fc_modules_dir/$kernel_ver" ]; then
      log "    Found kernel modules for version: $kernel_ver"
      
      # Create modules directory in staging
      busybox mkdir -p "${staging_dir}/lib/modules/$kernel_ver"
      # Copy vsock modules if they exist
      if [ -d "$fc_modules_dir/$kernel_ver/kernel/net/vmw_vsock" ]; then
        busybox mkdir -p "${staging_dir}/lib/modules/$kernel_ver/kernel/net"
        busybox cp -a "$fc_modules_dir/$kernel_ver/kernel/net/vmw_vsock" \
          "${staging_dir}/lib/modules/$kernel_ver/kernel/net/"
        log "      Copied vsock modules"
      fi
      
      # Copy modules.* files (needed for modprobe)
      busybox cp "$fc_modules_dir/$kernel_ver"/modules.* \
        "${staging_dir}/lib/modules/$kernel_ver/" 2>/dev/null || true
      log "      Copied module metadata files"
    else
      log "    WARNING: No kernel modules found in $fc_modules_dir"
    fi
  else
    log "    WARNING: Firecracker modules directory not found: $fc_modules_dir"
  fi

  
  # Create a simpler runcvm-vm-exec for Firecracker that doesn't depend on complex paths
  # This is called by dropbear when docker exec connects via SSH
  busybox cat > "${staging_dir}/.runcvm/guest/scripts/runcvm-vm-exec" << 'VMEXECEOF'
#!/bin/sh
# Simplified runcvm-vm-exec for Firecracker
# Called by dropbear SSH when docker exec connects
# Arguments: <uid:gid:groups> <cwd_encoded> <args_encoded> <env_encoded>

from_bin() {
  tr '\200\201\202\203\204\205' "\011\012\040\047\042\134"
}

uidgid="$1"
cwd_bin="$2"
args_bin="$3"
env_bin="$4"

# Decode working directory
cwd=$(printf '%s' "$cwd_bin" | from_bin)

# Change to working directory
cd "$cwd" 2>/dev/null || cd /

# Decode and execute the command
# The args are newline-separated after decoding
if [ -n "$args_bin" ]; then
  # Create a temporary script to handle the decoded args properly
  tmpscript="/tmp/exec-$$"
  printf '%s' "$args_bin" | from_bin > "$tmpscript.args"
  
  # Read args into positional parameters
  # We use a redirection loop which runs in current shell (usually) or use a FD
  # to avoid subshell if strictly POSIX but busybox sh handles this.
  
  # Reset positional parameters
  set --
  
  # Read the file line by line
  while IFS= read -r line || [ -n "$line" ]; do
    set -- "$@" "$line"
  done < "$tmpscript.args"
  
  rm -f "$tmpscript.args"
  
  # The first argument is the command (cmd)
  # But passing encoded args: <cmd> <arg1> <arg2>...
  # So $1 is cmd.
  
  # Execute the command
  exec "$@"

else
  # Single command fallback (should not happen with standard runcvm)
  # But if it does, assuming cmd comes from somewhere else?
  # The original logic parsed cmd from lines.
  # If args_bin is "cmd\narg1\narg2", then loop works.
  :
fi
  fi
else
  exec /bin/sh
fi
VMEXECEOF
  busybox chmod +x "${staging_dir}/.runcvm/guest/scripts/runcvm-vm-exec"
  log "    Created simplified runcvm-vm-exec"
  
  # Save network configuration for the VM
  # Save network configuration for all VMs
  # Generate network configuration files
  # This creates /.runcvm-network-eth0 (FC var format)
  generate_network_configs "$staging_dir"
  
  # CRITICAL: Copy raw network device files for k8s init script
  # runcvm-vm-init-k8s expects /.runcvm/network/devices/eth0
  if [ -d "/.runcvm/network" ]; then
       log "  Copying network config to rootfs..."
       busybox mkdir -p "${staging_dir}/.runcvm/network"
       busybox cp -a "/.runcvm/network/"* "${staging_dir}/.runcvm/network/"
       log "    Copied /.runcvm/network"
  fi
  
  # CRITICAL: Create /.runcvm/console file
  # runcvm-vm-init-k8s reads this to set up inittab
  echo "ttyS0" > "${staging_dir}/.runcvm/console"
  log "    Created /.runcvm/console (ttyS0)"

  # Inject Systemd Service for Entrypoint
  if [ "$RUNCVM_SYSTEMD" = "1" ] || [ "$RUNCVM_SYSTEMD" = "true" ]; then
    log "  Systemd enabled: Injecting runcvm-entrypoint service..."
    
    # 1. Create the runner script (decodes /.runcvm-entrypoint and execs it)
    local runner_script="${staging_dir}/usr/local/bin/runcvm-entrypoint-runner"
    busybox mkdir -p "${staging_dir}/usr/local/bin"
    
    cat > "$runner_script" << 'RUNNEREOF'
#!/bin/sh
# Wrapper to run the container entrypoint under systemd
set -e

if [ -f /.runcvm/entrypoint ]; then
  # Read entrypoint line by line into arg array
  set --
  while IFS= read -r line || [ -n "$line" ]; do
    set -- "$@" "$line"
  done < /.runcvm/entrypoint
   
  echo "RunCVM: Starting entrypoint: $@"
  
  # Check if entrypoint is just init/systemd (which we are already running)
  # If so, do nothing (success)
  case "$1" in
    /sbin/init|/lib/systemd/systemd|/usr/lib/systemd/systemd)
      echo "RunCVM: Entrypoint is init/systemd - skipping recursive execution"
      exit 0
      ;;
    *)
      exec "$@"
      ;;
  esac
else
  echo "RunCVM: No entrypoint found"
  exit 0
fi
RUNNEREOF
    busybox chmod +x "$runner_script"
    
    # 2. Create the Systemd Unit
    local unit_file="${staging_dir}/etc/systemd/system/runcvm-entrypoint.service"
    busybox mkdir -p "${staging_dir}/etc/systemd/system"
    
    cat > "$unit_file" << 'UNITEOF'
[Unit]
Description=RunCVM Container Entrypoint
After=network.target cloud-init.service
Wants=network.target

[Service]
Type=exec
ExecStart=/usr/local/bin/runcvm-entrypoint-runner
StandardOutput=journal+console
StandardError=journal+console
Restart=no
RemainAfterExit=yes
# Make sure we have a TTY if needed (optional, but good for interactive)
# TTYPath=/dev/ttyS0
# TTYReset=yes
# TTYVHangup=yes

[Install]
WantedBy=multi-user.target
UNITEOF

    # 3. Enable the service (symlink)
    busybox mkdir -p "${staging_dir}/etc/systemd/system/multi-user.target.wants"
    # Use relative symlink (safer for chroot/offline enablement)
    busybox ln -sf "../runcvm-entrypoint.service" "${staging_dir}/etc/systemd/system/multi-user.target.wants/runcvm-entrypoint.service"
    
    # --------------------------------------------------------
    # Inject Dropbear Service (for docker exec support)
    # --------------------------------------------------------
    log "  Systemd enabled: Injecting runcvm-dropbear service (port 22222)..."
    
    # Create keygen helper
    local dropbear_keygen="${staging_dir}/usr/local/bin/runcvm-dropbear-keygen"
    cat > "$dropbear_keygen" << 'KEYGENEOF'
#!/bin/sh
set -e
RUNCVM_GUEST="/.runcvm/guest"
RUNCVM_LD="$RUNCVM_GUEST/lib/ld"
DROPBEAR_DIR="/.runcvm/dropbear"
mkdir -p "$DROPBEAR_DIR"

if [ ! -f "$DROPBEAR_DIR/key" ]; then
  "$RUNCVM_LD" "$RUNCVM_GUEST/usr/bin/dropbearkey" -t ed25519 -f "$DROPBEAR_DIR/key" >/dev/null 2>&1
fi

# Re-generate EPKA config every time to be safe
KEY_PUBLIC=$("$RUNCVM_LD" "$RUNCVM_GUEST/usr/bin/dropbearkey" -y -f "$DROPBEAR_DIR/key" 2>/dev/null | grep ^ssh | cut -d' ' -f2)
echo '[{"user":"root","keytype":"ssh-ed25519","key":"'"$KEY_PUBLIC"'","options":"no-X11-forwarding","comments":""}]' > "$DROPBEAR_DIR/epka.json"
chmod 400 "$DROPBEAR_DIR/epka.json" "$DROPBEAR_DIR/key"
KEYGENEOF
    busybox chmod +x "$dropbear_keygen"

    # Create start helper
    local dropbear_start="${staging_dir}/usr/local/bin/runcvm-dropbear-start"
    cat > "$dropbear_start" << 'STARTEOF'
#!/bin/sh
RUNCVM_GUEST="/.runcvm/guest"
RUNCVM_LD="$RUNCVM_GUEST/lib/ld"
DROPBEAR_DIR="/.runcvm/dropbear"
EPKA_LIB="$RUNCVM_GUEST/tmp/dropbear/libepka_file.so"
SSHD_PORT=22222

exec "$RUNCVM_LD" "$RUNCVM_GUEST/usr/sbin/dropbear" -REF -p "$SSHD_PORT" \
  -A "$EPKA_LIB,$DROPBEAR_DIR/epka.json"
STARTEOF
    busybox chmod +x "$dropbear_start"

    # Create Service Unit
    local dropbear_unit="${staging_dir}/etc/systemd/system/runcvm-dropbear.service"
    cat > "$dropbear_unit" << 'DBUNITEOF'
[Unit]
Description=RunCVM Dropbear SSH
After=network.target

[Service]
Type=simple
ExecStartPre=/usr/local/bin/runcvm-dropbear-keygen
ExecStart=/usr/local/bin/runcvm-dropbear-start
StandardOutput=journal+console
StandardError=journal+console
Restart=always

[Install]
WantedBy=multi-user.target
DBUNITEOF

    # Enable Dropbear
    # Use relative symlink
    busybox ln -sf "../runcvm-dropbear.service" "${staging_dir}/etc/systemd/system/multi-user.target.wants/runcvm-dropbear.service"
    
    # Disable conflicting OpenSSH on port 22 (optional, but good practice if we don't want it)
    # Actually, we keep it for user convenience, as dropbear is on 22222.
    # But if we want to save resources:
    # busybox rm -f "${staging_dir}/etc/systemd/system/sshd.service"
    # busybox rm -f "${staging_dir}/etc/systemd/system/ssh.service" 2>/dev/null || true
    # For now, leave OpenSSH enabled as users might want standard SSH access.
  fi

  # Save resolv.conf
  if [ -f "/etc/resolv.conf" ]; then
    busybox cp /etc/resolv.conf "${staging_dir}/.runcvm-resolv.conf"
  fi
  
  # Step 2: Calculate rootfs size (Minimal)
  # Just enough to hold files + small buffer (e.g. 100MB)
  # We will resize it LATER for the runtime instance.
  local rootfs_size=$(( staged_size + 150 ))
  [ "$rootfs_size" -lt 64 ] && rootfs_size=64
  
  # CRITICAL: BRIDGE ALIAS LOGIC REMOVED (transparent L2 bridging used instead)
  # See docs/kubernetes/networking-debug-summary.md for context.
  : 

  log "Step 2/3: Creating rootfs image..."
  log "  Rootfs size: ${rootfs_size}MB (${staged_size}MB content + overhead)"
  
  # Check overlay disk space
  local overlay_avail=$(busybox df -m / 2>/dev/null | busybox awk 'NR==2 {print $4}')
  log "  Overlay (/) available: ${overlay_avail}MB"
  
  if [ "$overlay_avail" -lt "$rootfs_size" ]; then
    busybox rm -rf "$staging_dir"
    error "Insufficient disk space on overlay (need ${rootfs_size}MB, have ${overlay_avail}MB)"
  fi
  
  log "DEBUG: Verifying staging directory ($staging_dir) before rootfs creation:"
  ls -la "$staging_dir" | while read line; do log "  $line"; done
  log "DEBUG: Verifying staging /.runcvm content:"
  ls -la "$staging_dir/.runcvm" | while read line; do log "  $line"; done
  
  # Create rootfs from staging directory
  log "  Source: $staging_dir"  
  log "  Destination: $rootfs_path"
  
  local mke2fs_start=$(busybox date +%s)
  
  if ! create_rootfs_from_dir "$staging_dir" "$rootfs_path" "$rootfs_size"; then
    log "mke2fs debug info:"
    log "  Staged content size: $(busybox du -sm "$staging_dir" | busybox cut -f1)MB"
    log "  Image size: ${rootfs_size}MB"
    busybox rm -rf "$staging_dir"
    error "Failed to create rootfs"
  fi
  
  local mke2fs_end=$(busybox date +%s)
  local mke2fs_time=$(( mke2fs_end - mke2fs_start ))
  
  # Cleanup staging
  log "  Cleaning up staging area..."
  busybox rm -rf "$staging_dir"
  
  local final_size=$(busybox du -sm "$rootfs_path" 2>/dev/null | busybox cut -f1)
  log "  Rootfs ready: ${final_size}MB (created in ${mke2fs_time}s)"

  # if command -v debugfs >/dev/null 2>&1; then
  #    log "DEBUG: Checking rootfs content using debugfs:"
  #    debugfs -R "ls -l /" "$rootfs_path" | while read line; do log "  $line"; done
  #    log "DEBUG: Checking /.runcvm content in rootfs:"
  #    debugfs -R "ls -l /.runcvm" "$rootfs_path" | while read line; do log "  $line"; done
  # fi

  # CACHE UPDATE (Save SMALL image)
  if [ "$use_cache" = "1" ]; then
     log "Updating cache: $cache_file"
     if cp --sparse=always "$rootfs_path" "$cache_file" 2>/dev/null; then
        :
     else
        busybox cp "$rootfs_path" "$cache_file"
     fi
  fi
  
  # Expand rootfs for Runtime
  TARGET_SIZE="${RUNCVM_ROOTFS_SIZE:-256M}"
  log "Resizing rootfs to $TARGET_SIZE..."
  if command -v truncate >/dev/null 2>&1; then
     truncate -s "$TARGET_SIZE" "$rootfs_path"
  else
     local size_bytes=$(( 2 * 1024 * 1024 * 1024 ))
     busybox dd of="$rootfs_path" bs=1 count=0 seek="$size_bytes" 2>/dev/null || true
  fi
  if command -v resize2fs >/dev/null 2>&1; then
     resize2fs "$rootfs_path" >/dev/null 2>&1
  fi

fi # End Cache Miss/No Cache logic

  
  # Determine kernel path - using Firecracker kernel Image (uncompressed)
  # The kernel is copied to /opt/runcvm/kernels/firecracker/vmlinux in Dockerfile
  # which becomes /.runcvm/guest/kernels/firecracker/vmlinux at runtime
  local fc_default_kernel="$RUNCVM_GUEST/kernels/firecracker/vmlinux"
  local kernel_path="${RUNCVM_FC_KERNEL_PATH:-$fc_default_kernel}"
  
  if [ ! -f "$kernel_path" ]; then
    error "Firecracker kernel not found: $kernel_path"
  fi
  
  # DEBUG: Show kernel file details
  log "DEBUG: Kernel file information:"
  log "  Path: $kernel_path"
  log "  Size: $(busybox ls -lh "$kernel_path" 2>/dev/null | busybox awk '{print $5}')"
  log "  Modified: $(busybox ls -l "$kernel_path" 2>/dev/null | busybox awk '{print $6, $7, $8}')"
  if command -v sha256sum >/dev/null 2>&1; then
    log "  SHA256: $(sha256sum "$kernel_path" 2>/dev/null | busybox cut -d' ' -f1)"
  fi
  
  # Build boot arguments - use /init which we created in the rootfs
  # Note: 9p.debug=0xff enables verbose 9P logging to debug transport issues
  local boot_args="init=/init console=ttyS0 reboot=k panic=1 pci=off root=/dev/vda rw"
  if [ -n "$RUNCVM_KERNEL_ARGS" ]; then
      boot_args="$boot_args $RUNCVM_KERNEL_ARGS"
  fi
  # Add 'quiet' to suppress kernel boot messages when not in DEBUG mode
  if [ "$RUNCVM_LOG_LEVEL" != "DEBUG" ]; then
    boot_args="$boot_args quiet loglevel=0 systemd.show_status=false"
  fi
  
  # Append container command for init to execute
  if [ -n "$CMD_ARGS" ]; then
    boot_args="$boot_args -- $CMD_ARGS"
  fi

  
  # Parse memory size (remove 'M' suffix if present)
  local mem_mb="${RUNCVM_MEM_SIZE%M}"
  [ -z "$mem_mb" ] && mem_mb=768
  
  # Parse CPU count
  local vcpu_count="${RUNCVM_CPUS:-1}"
  [ "$vcpu_count" -le 0 ] && vcpu_count=$(busybox nproc)
  
  log "Step 3/3: Starting Firecracker VM..."
  log "  Kernel: $kernel_path"
  log "  Config: $vcpu_count vCPUs, ${mem_mb}MB RAM"
  
  # Note: /.runcvm keys are copied into rootfs during creation (see earlier COPY steps)
  # Firecracker does not support bind mounts, so we rely on copying or NFS.

  
  # Build network config if available
  local network_interfaces_json=""
  local is_host_mode=0
  [ -f "/.runcvm/network/host_mode" ] && is_host_mode=1
  
  # Iterate over all defined network devices
  # We use a sorted list to ensure eth0 comes first (important for default gateway)
  if [ -d "/.runcvm/network/devices" ]; then
    for device_file in $(busybox ls /.runcvm/network/devices/* | busybox sort); do
      # Skip the 'default' symlink to avoid duplicates
      if [ "$(busybox basename "$device_file")" = "default" ]; then
        continue
      fi
      
      read DOCKER_IF DOCKER_IF_MAC DOCKER_IF_MTU DOCKER_IF_IP DOCKER_IF_IP_NETPREFIX DOCKER_IF_IP_GW < "$device_file"
      
      # MAC ADDRESS SWAPPING (Critical for Cloud/CNI)
      # We must use the AUTHORIZED MAC ($DOCKER_IF_MAC) inside the Guest.
      # But we can't have duplicate MACs on the bridge (Host eth0 vs Guest TAP).
      # SO: We swap them. Host eth0 gets the dummy MAC, Guest gets the Real MAC.
      
      local fc_mac="$DOCKER_IF_MAC"
      local host_dummy_mac=$(echo "$DOCKER_IF_MAC" | busybox sed 's/^..:..:../AA:FC:00/')
      
      # If Host eth0 still has the Real MAC, swap it!
      # We check current MAC.
      local current_mac=$(cat /sys/class/net/$DOCKER_IF/address)
      if [ "$current_mac" = "$DOCKER_IF_MAC" ]; then
         log "  Swapping MACs for $DOCKER_IF to avoid bridge collision..."
         /.runcvm/guest/sbin/ip link set dev "$DOCKER_IF" down
         /.runcvm/guest/sbin/ip link set dev "$DOCKER_IF" address "$host_dummy_mac"
         /.runcvm/guest/sbin/ip link set dev "$DOCKER_IF" up
      fi

      # ENABLE PROMISCUOUS MODE (Critical for Nested VM / Bridge)
      # The Bridge (br-eth0) needs to see traffic for the VM's MAC.
      # Since eth0 is a slave, it should technically be promisc automatically, 
      # but in some cloud/nested envs, we must force it.
      log "  Enabling promiscuous mode on $DOCKER_IF..."
      /.runcvm/guest/sbin/ip link set dev "$DOCKER_IF" promisc on
      
      local tap_name=""
      
      if [ "$is_host_mode" = "1" ] && [ "$DOCKER_IF" = "eth0" ]; then
        # Host Mode (NAT/TAP)
        # Entrypoint already created 'tap0' for us
        tap_name="tap0"
        log "  Network ($DOCKER_IF): Host Mode, using existing $tap_name"
        
        # Ensure it's up
        ip link set dev "$tap_name" up mtu "${DOCKER_IF_MTU:-1500}"
        
      else
        # Bridge Mode (Docker/Kubernetes)
        tap_name="tap-$DOCKER_IF"
        local bridge_name="br-$DOCKER_IF"
        
        log "  Network ($DOCKER_IF): $tap_name ($fc_mac) -> $bridge_name"
        
        # Ensure IP Forwarding is enabled (CRITICAL for routing/NAT)
        if [ -w /proc/sys/net/ipv4/ip_forward ]; then
           echo 1 > /proc/sys/net/ipv4/ip_forward
        else
           # Try sysctl
           sysctl -w net.ipv4.ip_forward=1 >/dev/null 2>&1 || true
        fi
        
        # Create TAP device for Firecracker if it doesn't exist


        if ! /.runcvm/guest/sbin/ip link show "$tap_name" >/dev/null 2>&1; then
           log "    Creating TAP device $tap_name..."
           /.runcvm/guest/sbin/ip tuntap add dev "$tap_name" mode tap
        fi
        
        # Add TAP to the bridge
        log "    Adding $tap_name to bridge $bridge_name..."
        /.runcvm/guest/sbin/ip link set dev "$tap_name" master "$bridge_name"
        
        # Ensure Bridge is UP (Critical if we skipped creation or if down)
        log "    Ensuring bridge $bridge_name is UP..."
        /.runcvm/guest/sbin/ip link set dev "$bridge_name" up
        
        # Set MTU and bring up
        /.runcvm/guest/sbin/ip link set dev "$tap_name" up mtu "${DOCKER_IF_MTU:-1500}"
        
        # Add IP alias to bridge to allow container-to-VM communication (e.g. docker exec)
        # We pick 172.17.0.254 (or derived from network) to be on the same subnet as VM.
        # This ensures ARP works correctly without needing special routes in VM.
        # Note: If DOCKER_IF_IP_NETPREFIX=16, we use .254.
        # We need to construct a valid IP.
        # Hardcoding .254 might conflict if the subnet is small (/24).
        # But Docker default is /16.
        # Safer: Add address 169.254.1.1/32 to bridge here explicitly if not done elsewhere?
        # NO, we want SAME SUBNET.
        # Let's derive it. echo "172.17.255.254"
        # For now, simplistic approach: Force 169.254.1.1 is already there. Add 10.0.0.1?
        # If we use 169.254.1.1 on bridge and added route to VM, it SHOULD work.
        # Why did it fail?
        # Let's try adding the Docker Gateway IP (172.17.0.1) to the bridge? NO conflict.
        # Let's add 172.17.x.254.
        # Constructing IP logic is complex in shell.
        # I will stick to my "Route Injection" strategy BUT make sure it works.
        # Actually... if I simply add "ip addr add $DOCKER_IF_IP/32 ...".
        # No, that conflicts.
        #
        # Re-thinking: The route injection failed.
        # Let's try adding 169.254.1.2/32 to the VM side (in init script) as I planned before?
        # No I added route.
        # Let's try ADDING IP to Bridge: 172.17.255.254/16 (assuming /16).
        # If prefix is /24, then 172.17.0.254/24.
        
        # Implementation:
        net_prefix="${DOCKER_IF_IP_NETPREFIX:-16}"
        
        # DEPRECATED: alias_ip assignment is removed to avoid interrupting L2 bridging.
        log "    Skipping bridge alias assignment (Transparent L2 mode enabled)"
        
        # SNAT cleanup should still be done if relevant, but the IP deletion is the main point.

        # Restore Default Route (lost when flushing IPs)
        # This ensures the container namespace (host side) has internet access via the bridge
            if [ -n "$DOCKER_IF_IP_GW" ]; then
                log "    Restoring default route via $DOCKER_IF_IP_GW dev $bridge_name"
                /.runcvm/guest/sbin/ip route add default via "$DOCKER_IF_IP_GW" dev "$bridge_name" 2>/dev/null || true
            fi
      
        # Debug: show bridge and tap status
        if [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]; then
          log "    Bridge status:"
          /.runcvm/guest/sbin/ip link show "$bridge_name" 2>&1 | busybox sed 's/^/      /'
          log "    TAP status:"
          /.runcvm/guest/sbin/ip link show "$tap_name" 2>&1 | busybox sed 's/^/      /'
        fi
        
        # Add MASQUERADE rule for outbound traffic from the VM (for the default gateway interface)
        # This allows the VM to reach external networks (internet)
        if [ -n "$DOCKER_IF_IP" ] && [ "$DOCKER_IF_IP" != "-" ]; then
             log "    Adding NAT MASQUERADE rule for $DOCKER_IF..."
             if [ -n "$RUNCVM_GUEST" ] && [ -d "$RUNCVM_GUEST/usr/lib/xtables" ]; then
               XTABLES_LIBDIR=$RUNCVM_GUEST/usr/lib/xtables xtables-nft-multi iptables \
                 -t nat -A POSTROUTING -s "$DOCKER_IF_IP" -o "$DOCKER_IF" -j MASQUERADE 2>/dev/null || true
             else
               iptables -t nat -A POSTROUTING -s "$DOCKER_IF_IP" -o "$DOCKER_IF" -j MASQUERADE 2>/dev/null || true
             fi
        fi
        # ============================================================
        # START NFS DAEMON (Inside Container Namespace)
        # ============================================================
        if [ -n "$RUNCVM_NFS_HOST_EXPORTS" ] && [ -n "$RUNCVM_CONTAINER_ID" ]; then
            log "    Starting HOST unfsd for NFS volumes (inside namespace)..."
            if [ -x "$RUNCVM_GUEST/scripts/runcvm-nfsd" ]; then
                # We are already in the correct network namespace, so we pass 0 for netns_pid
                RUNCVM="$RUNCVM_GUEST" "$RUNCVM_GUEST/scripts/runcvm-nfsd" start \
                    "$RUNCVM_CONTAINER_ID" \
                    "$RUNCVM_NFS_HOST_EXPORTS" \
                    "multi" \
                    "${RUNCVM_NFS_PORT:-1024}" \
                    "${RUNCVM_NFS_UID:-0}" \
                    "${RUNCVM_NFS_GID:-0}" \
                    "0" 2>&1 | busybox sed 's/^/      /'
            else
                log "    [WARNING] $RUNCVM_GUEST/scripts/runcvm-nfsd not found, cannot start NFS"
            fi
        fi
      
      # Build JSON object for this interface
      local iface_json="{\"iface_id\":\"$DOCKER_IF\",\"guest_mac\":\"$fc_mac\",\"host_dev_name\":\"$tap_name\"}"
      
      if [ -z "$network_interfaces_json" ]; then
        network_interfaces_json="$iface_json"
      else
        network_interfaces_json="$network_interfaces_json,$iface_json"
      fi
      
    done
  fi # if devices dir exists

  # Wrap in array structure if we found interfaces
  if [ -n "$network_interfaces_json" ]; then
    network_config=",\"network-interfaces\":[$network_interfaces_json]"
  fi
  # ==========================================================================
  # INJECT NETWORK CONFIG INTO ROOTFS
  # ==========================================================================
  # Since network config is generated AFTER rootfs creation, we must inject it.
  # This fixes the IP mismatch where Cached/New rootfs doesn't have the new config.
  log "Injecting updated configuration into rootfs..."
  if command -v debugfs >/dev/null 2>&1; then
    # We must inject the generated /init script because it contains the embedded IP configuration.
    # Cached rootfs has a stale /init with old IP.
    if [ -f "$staging_dir/init" ]; then
        debugfs -w -R "rm /init" "$rootfs_path" >/dev/null 2>&1 || true
        if ! debugfs -w -R "write $staging_dir/init /init" "$rootfs_path" >/dev/null 2>&1; then
             log "  [WARNING] Failed to inject /init into rootfs"
        else
             log "  Injected /init"
        fi
    fi
    
    # Also inject other files if present (e.g. if we switched to file-based config later)
    for net_conf in "$staging_dir"/.runcvm-network-*; do
      [ -f "$net_conf" ] || continue
      local fname=$(busybox basename "$net_conf")
      debugfs -w -R "rm /$fname" "$rootfs_path" >/dev/null 2>&1 || true
      debugfs -w -R "write $net_conf /$fname" "$rootfs_path" >/dev/null 2>&1
    done
  else
    log "  [WARNING] debugfs missing, cannot inject network config! Networking may fail."
  fi

  # ==========================================================================
  # LOOPBACK CONNECTION SETUP (For docker exec) - HOST SIDE
  # ==========================================================================
  # We use socat as a user-space proxy to forward 127.0.0.1:22222 -> VM:22222
  # This works without privileges (unlike iptables/sysctl) and supports concurrency.
  
  log INFO "Configuring Loopback Access (socat proxy) for docker exec..."
  SSHD_PORT=22222 # Ensure variable is available in this scope
  
  # We need the VM's bridge IP (from the Loopback Alias we set up earlier)
  # Actually, the IP we want to connect TO is the VM's IP.
  # The VM's IP is DOCKER_IF_IP (passed via env or file).
  # BUT we are on the host (container). The VM is connected via TAP.
  # The VM IP is reachable via the bridge.
  
  LOOPBACK_TARGET_IP=""
  # Try to reuse the IP determined during network setup
  if [ -n "$DOCKER_IF_IP" ] && [ "$DOCKER_IF_IP" != "-" ]; then
     LOOPBACK_TARGET_IP="$DOCKER_IF_IP"
     log INFO "  Using DOCKER_IF_IP ($LOOPBACK_TARGET_IP) for loopback proxy"
  fi
  
  if [ -z "$LOOPBACK_TARGET_IP" ]; then
    # Dynamic detection: Iterate known network devices to find a valid IP
    # Prioritize eth0, then default, then any other interface to ensure deterministic behavior
    # in multi-network scenarios.
    
    # 1. Try eth0 explicitly (Primary Interface)
    if [ -f "/.runcvm/network/devices/eth0" ]; then
        read _ _ _ CHECK_IP _ _ < "/.runcvm/network/devices/eth0"
        if [ -n "$CHECK_IP" ] && [ "$CHECK_IP" != "-" ]; then
             LOOPBACK_TARGET_IP="$CHECK_IP"
             log INFO "  Using eth0 IP: $LOOPBACK_TARGET_IP"
        fi
    fi

    # 2. Try 'default' if eth0 failed
    if [ -z "$LOOPBACK_TARGET_IP" ] && [ -f "/.runcvm/network/devices/default" ]; then
        read _ _ _ CHECK_IP _ _ < "/.runcvm/network/devices/default"
         if [ -n "$CHECK_IP" ] && [ "$CHECK_IP" != "-" ]; then
             LOOPBACK_TARGET_IP="$CHECK_IP"
             log INFO "  Using default IP: $LOOPBACK_TARGET_IP"
        fi
    fi

    # 3. Fallback to any other interface
    if [ -z "$LOOPBACK_TARGET_IP" ]; then
        for dev_file in /.runcvm/network/devices/*; do
            [ -f "$dev_file" ] || continue
            # Skip eth0 and default as we already checked them
            local bname=$(busybox basename "$dev_file")
            [ "$bname" = "eth0" ] || [ "$bname" = "default" ] && continue
            
            read _ _ _ CHECK_IP _ _ < "$dev_file"
            if [ -n "$CHECK_IP" ] && [ "$CHECK_IP" != "-" ]; then
                LOOPBACK_TARGET_IP="$CHECK_IP"
                log INFO "  Found fallback IP from $dev_file: $LOOPBACK_TARGET_IP"
                break
            fi
        done
    fi
  fi
  
  if [ -n "$LOOPBACK_TARGET_IP" ]; then
    log INFO "  Starting socat proxy: 127.0.0.1:$SSHD_PORT -> $LOOPBACK_TARGET_IP:$SSHD_PORT"
    
    # Debug: Check connectivity to VM IP
    log INFO "  Verifying connectivity to $LOOPBACK_TARGET_IP..."
    if busybox ping -c 1 -W 1 "$LOOPBACK_TARGET_IP" >/dev/null 2>&1; then
       log INFO "  ✓ VM IP is reachable"
    else
       log INFO "  ✗ VM IP is NOT reachable (yet)"
    fi

    # Check if socat is available
    if command -v socat >/dev/null 2>&1; then
       # Start socat in background
       # TCP-LISTEN:22222,bind=127.0.0.1,fork,reuseaddr -> TCP:$VM_IP:22222
       # CRITICAL: Log errors to console
       socat TCP-LISTEN:"$SSHD_PORT",bind=127.0.0.1,fork,reuseaddr TCP:"$LOOPBACK_TARGET_IP":"$SSHD_PORT" 2>&1 &
       SOCAT_PID=$!
       log INFO "  socat started (PID: $SOCAT_PID)"
    elif [ -x "/.runcvm/guest/usr/bin/socat" ]; then
       # Fallback to bundled socat if not in PATH (e.g. if runcvm-ctr-defaults didnt alias it correctly yet?)
       # Note: We rely on alias or absolute path.
       /.runcvm/guest/usr/bin/socat TCP-LISTEN:"$SSHD_PORT",bind=127.0.0.1,fork,reuseaddr TCP:"$LOOPBACK_TARGET_IP":"$SSHD_PORT" 2>&1 &
       SOCAT_PID=$!
       log INFO "  socat started (PID: $SOCAT_PID) via absolute path"
    else
       log INFO "  Warning: socat not found, loopback connection strategy might fail"
    fi
  else
    log INFO "  Warning: Could not determine VM IP for loopback proxy"
  fi
  # FORCE ENABLE vsock for debugging - always add the vsock device
  local vsock_config=',"vsock":{"guest_cid":3,"uds_path":"/run/firecracker.vsock"}'
  log "  vsock: FORCE ENABLED (debugging)"
  log "    guest_cid: 3"
  log "    uds_path: /run/firecracker.vsock"
  

  # Build balloon config if enabled
  # amount_mib: 0 means the balloon is empty (guest has full memory)
  # deflate_on_oom: true allows the guest to reclaim memory if it runs out
  # stats_polling_interval_s: 1 enables memory stats reporting
  local balloon_config=""
  if [ "${RUNCVM_ENABLE_BALLOON:-true}" = "true" ]; then
    local balloon_size="${RUNCVM_BALLOON_SIZE_MIB:-0}"
    log "  Memory Ballooning: ENABLED (Size: ${balloon_size}MiB)"
    balloon_config=',"balloon":{"amount_mib":'$balloon_size',"deflate_on_oom":true,"stats_polling_interval_s":1}'
  else
    log "  Memory Ballooning: DISABLED (default)"
  fi

  # Create Firecracker config file
  # Create Firecracker config file


  generate_firecracker_config \
    "$kernel_path" \
    "$boot_args" \
    "$rootfs_path" \
    "$vcpu_count" \
    "$mem_mb" \
    "$network_config" \
    "$vsock_config" \
    "$balloon_config" \
    "$FIRECRACKER_CONFIG"

  # ALWAYS show Firecracker config for debugging vsock issue
  log INFO "=== FIRECRACKER CONFIG (checking vsock) ==="
  busybox cat "$FIRECRACKER_CONFIG" | while read line; do log "  $line"; done
  log INFO "=== END FIRECRACKER CONFIG ==="

  # Verify vsock is in config
  if busybox grep -q "\"vsock\"" "$FIRECRACKER_CONFIG"; then
    log "✓ vsock is PRESENT in Firecracker config"
  else
    log "✗ vsock is MISSING from Firecracker config!"
  fi

  # ============================================================
  # HOST TERMINAL CONFIGURATION
  # ============================================================
  # NOTE: We tried stty raw but it may trigger DSR queries.
  # The working test script runs Firecracker without modifying host terminal.
  # Let's try the same approach here.
  # ============================================================
  
  log "Starting Firecracker..."
  local fc_error_log=""

  if [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]; then
    # In DEBUG mode, show everything directly
    "$FIRECRACKER_BIN" --no-api --config-file "$FIRECRACKER_CONFIG"
  else
    # In non-DEBUG mode, suppress stderr (logs, banner) by redirecting to a temp file
    # This preserves stdout (console) interactivity while hiding the banner
    fc_error_log="/tmp/firecracker-error-$$.log"
    
    # Run Firecracker: stdout -> terminal (interactive), stderr -> log file
    # API Socket: /.runcvm/sockets/firecracker.socket (exposed to host)
    "$FIRECRACKER_BIN" --api-sock "/.runcvm/sockets/firecracker.socket" --config-file "$FIRECRACKER_CONFIG" 2> "$fc_error_log"
  fi
  
  local exit_code=$?

  # If failed and we have a log, show it
  if [ "$exit_code" -ne 0 ] && [ -n "$fc_error_log" ] && [ -f "$fc_error_log" ]; then
    cat "$fc_error_log" >&2
  fi
  
  # Cleanup temp log
  [ -n "$fc_error_log" ] && rm -f "$fc_error_log"
  


  log "Firecracker exited with code $exit_code"
  
  # Cleanup
  busybox rm -f "$rootfs_path" "$FIRECRACKER_CONFIG"
  
  return $exit_code
}

main "$@"
