#!/.runcvm/guest/bin/bash

# RunCVM Firecracker Launcher
# Launches a Firecracker microVM using config file mode (for console output)
#
# STRATEGY: Copy container files to /dev/shm, then create rootfs on overlay
# This avoids the self-copy problem where mke2fs would copy the rootfs into itself

set -o errexit -o pipefail
# set -x # Debug tracing disabled for production speed

# Load original environment
. /.runcvm/config

# Load defaults
. $RUNCVM_GUEST/scripts/runcvm-ctr-defaults && unset PATH

FIRECRACKER_BIN="$RUNCVM_GUEST/sbin/firecracker"
FIRECRACKER_CONFIG="/run/.firecracker-config.json"
NFS_CONFIG="/.runcvm/nfs-mounts"
NFS_PORT_MIN=1000
NFS_PORT_MAX=1050

# ============================================================
# LOGGING SYSTEM
# Severity levels: DEBUG, INFO, ERROR, OFF
# Control via: RUNCVM_LOG_LEVEL environment variable
# ============================================================

# Default log level (can be overridden by environment)
RUNCVM_LOG_LEVEL="${RUNCVM_LOG_LEVEL:-OFF}"

# Log severity levels (numeric for comparison)
declare -A LOG_LEVELS=(
  [DEBUG]=0
  [INFO]=1
  [LOG]=1      # Alias for INFO
  [ERROR]=2
  [OFF]=999
)

# Get numeric level for current log level (default to OFF=999)
CURRENT_LOG_LEVEL="${LOG_LEVELS[$RUNCVM_LOG_LEVEL]:-999}"

# Core logging function
_log() {
  local severity="$1"
  shift
  local message="$*"
  local severity_level="${LOG_LEVELS[$severity]:-1}"
  
  # Only log if severity meets threshold
  if [ "$severity_level" -ge "$CURRENT_LOG_LEVEL" ]; then
    echo "[$(busybox date '+%Y-%m-%d %H:%M:%S')] [RunCVM-FC] [$severity] $message" >&2
  fi
}

# Convenience functions for different severity levels
log_debug() {
  _log DEBUG "$@"
}

log_info() {
  _log INFO "$@"
}

log() {
  # Default log function - maps to INFO for backward compatibility
  _log INFO "$@"
}

log_error() {
  _log ERROR "$@"
}

error() {
  # ALWAYS log errors to stderr, regardless of log level
  echo "[$(busybox date '+%Y-%m-%d %H:%M:%S')] [RunCVM-FC] [ERROR] $@" >&2
  exit 1
}



setup_nfs_volumes() {
  # NFS volume sync using HOST-SIDE unfsd
  # Architecture:
  #   Host: runcvm-runtime starts unfsd daemon and sets RUNCVM_NFS_VOLUMES env var
  #   Container: reads env var and writes NFS config for guest VM
  #   Guest: mounts via NFS client (kernel built-in)
  #
  # RUNCVM_NFS_VOLUMES format: src:dst:port|src2:dst2:port2|...
  
  local nfs_config="$NFS_CONFIG"
  
  # DEBUG: Show what we have (only if log level is DEBUG)
  if [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]; then
    echo "[DEBUG] setup_nfs_volumes checking env var" >&2
    echo "[DEBUG] RUNCVM_NFS_VOLUMES=${RUNCVM_NFS_VOLUMES:-<not set>}" >&2
  fi
  
  # Read from environment variable (set by runcvm-runtime)
  if [ -n "$RUNCVM_NFS_VOLUMES" ]; then
    > "$nfs_config"
    
    log INFO "Setting up NFS volumes from RUNCVM_NFS_VOLUMES env"
    
    # Parse pipe-separated entries
    echo "$RUNCVM_NFS_VOLUMES" | busybox tr '|' '\n' | while IFS=: read -r src dst port; do
      if [ -z "$src" ]; then continue; fi
      if [ -z "$port" ]; then port="2049"; fi
      
      log INFO "  Volume: $src -> $dst (port $port)"
      if [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]; then
         echo "[DEBUG] NFS config line: $src:$dst:$port" >&2
      fi
      
      # Write config for guest (format: src:dst:port)
      echo "$src:$dst:$port" >> "$nfs_config"
    done
    
    log INFO "  NFS config written to $nfs_config"
    if [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]; then
      echo "[DEBUG] Final nfs_config: $(busybox cat $nfs_config)" >&2
    fi
    
  else
    log INFO "RUNCVM_NFS_VOLUMES not set, skipping NFS volume setup"
  fi
}

cleanup_nfs_volumes() {
  # NFS cleanup is handled by runcvm-runtime on container stop
  # Nothing to do here
  :
}

trap cleanup_nfs_volumes EXIT SIGTERM SIGINT

# Create rootfs image from a source directory
create_rootfs_from_dir() {
  local source_dir="$1"
  local image_path="$2"
  local size_mb="$3"
  
  log "Creating ext4 rootfs: $image_path (${size_mb}MB) from $source_dir"
  
  # Create sparse file
  if ! busybox truncate -s "${size_mb}M" "$image_path"; then
    error "Failed to create sparse file"
  fi
  
  # Create ext4 filesystem populated with source directory contents
  if [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]; then
    # Show mke2fs output in debug mode
    if ! mke2fs -F -t ext4 -E root_owner=0:0 -d "$source_dir" "$image_path" 2>&1; then
      log "mke2fs failed"
      busybox rm -f "$image_path"
      return 1
    fi
  else
    # Silent mode - redirect to /dev/null
    if ! mke2fs -F -t ext4 -E root_owner=0:0 -d "$source_dir" "$image_path" >/dev/null 2>&1; then
      log_error "mke2fs failed"
      busybox rm -f "$image_path"
      return 1
    fi
  fi
  log "Rootfs created successfully"
  return 0
}

# Main function
main() {
  log "Starting Firecracker microVM launcher..."
  # ==========================================================================
  # STEP 0: Extract volumes and start 9P servers BEFORE creating rootfs
  # ==========================================================================
  


  setup_nfs_volumes

  log_debug "NFS_CONFIG=$NFS_CONFIG"
  if [ -f "$NFS_CONFIG" ]; then
    log_debug "Contents of $NFS_CONFIG:"
    busybox cat "$NFS_CONFIG" 2>&1 | while read line; do log_debug "  $line"; done
  else
    log_debug "NFS_CONFIG file does not exist (no volumes)"
  fi
  
  # Check for Firecracker binary
  if [ ! -x "$FIRECRACKER_BIN" ]; then
    error "Firecracker binary not found: $FIRECRACKER_BIN"
  fi
  
  # ==========================================================================
  # CACHE MAINTENANCE (LRU Eviction)
  # ==========================================================================
  # Clean up old cache files to prevent disk fill-up
  # Delete files accessed more than 24 hours ago (-atime +1) or modified (-mtime +1)
  if [ -d "/.runcvm/cache" ]; then
    log_debug "Running cache maintenance..."
    # Run in background to not block boot
    (
      find /.runcvm/cache -name "*.ext4" -type f -mtime +1 -delete 2>/dev/null || true
    ) &
  fi

  # ==========================================================================
  # STRATEGY: Rootfs Creation with Caching
  # ==========================================================================
  
  local rootfs_path="/rootfs.ext4"
  local entrypoint_file="/.runcvm/entrypoint"
  local cache_key="${RUNCVM_CACHE_KEY}"
  local cache_file=""
  local use_cache=0
  
  if [ -n "$cache_key" ] && [ -d "/.runcvm/cache" ]; then
    cache_file="/.runcvm/cache/${cache_key}.ext4"
    use_cache=1
    log "Cache Key: $cache_key"
  fi

  # CACHE HIT CHECK
  if [ "$use_cache" = "1" ] && [ -f "$cache_file" ]; then
    log "Cache HIT! Using cached rootfs: $cache_file"
    
    # Fast copy small image
    if cp --sparse=always "$cache_file" "$rootfs_path" 2>/dev/null; then
       :
    else
       busybox cp "$cache_file" "$rootfs_path"
    fi
    
    # INJECT CURRENT ENTRYPOINT
    # The cached rootfs has a stale entrypoint. We must overwrite it with the current one.
    if command -v debugfs >/dev/null 2>&1 && [ -f "$entrypoint_file" ]; then
       # Run debugfs to update entrypoint (surpress output unless error)
       debugfs -w -R "rm /.runcvm-entrypoint" "$rootfs_path" >/dev/null 2>&1 || true
       if ! debugfs -w -R "write $entrypoint_file /.runcvm-entrypoint" "$rootfs_path" >/dev/null 2>&1; then
          log "  [WARNING] Failed to update entrypoint in rootfs"
       fi
    fi
    
    # Expand to full size
    # Allow user to override via env
    TARGET_SIZE="${RUNCVM_ROOTFS_SIZE:-256M}"
    
    log "Resizing rootfs to $TARGET_SIZE..."
    if command -v truncate >/dev/null 2>&1; then
       truncate -s "$TARGET_SIZE" "$rootfs_path"
    else
       # Fallback to dd (2GB = 2147483648 bytes)
       # Busybox dd might not support G suffix, so use bytes
       local size_bytes=$(( 2 * 1024 * 1024 * 1024 ))
       busybox dd of="$rootfs_path" bs=1 count=0 seek="$size_bytes" 2>/dev/null || true
    fi
    if command -v resize2fs >/dev/null 2>&1; then
       resize2fs "$rootfs_path" >/dev/null 2>&1
    else
       log "WARNING: resize2fs not found, rootfs will be small!"
    fi
    
    log "Rootfs ready (from cache, resized)."
    
  else
    if [ "$use_cache" = "1" ]; then
       log "Cache MISS. Creating new rootfs..."
    else
       log "Cache disabled or unavailable. Creating rootfs from scratch..."
    fi

    # EXISTING LOGIC: Copy container files to /dev/shm, then create rootfs on overlay
    # This avoids the self-copy problem where mke2fs would copy the rootfs into itself
    
    # Use /tmp for staging instead of /dev/shm to avoid ENOSPC on large images
    local staging_dir="/tmp/rootfs-staging"
    
    # Calculate source size (excluding .runcvm which has RunCVM binaries)
    log "Calculating container size (excluding .runcvm)..."
    
    local source_size=$(busybox du -sm --exclude='.runcvm' "$RUNCVM_VM_MOUNTPOINT" 2>/dev/null | busybox cut -f1)
    [ -z "$source_size" ] && source_size=100
    
    log "Container filesystem size: ${source_size}MB"
    
    # Check if /dev/shm has enough space for staging
    local shm_avail=$(busybox df -m /dev/shm 2>/dev/null | busybox awk 'NR==2 {print $4}')
    local shm_needed=$(( source_size + 50 ))  # Add 50MB buffer
    
    log "/dev/shm available: ${shm_avail}MB, need for staging: ${shm_needed}MB"
    
    if [ "$shm_avail" -lt "$shm_needed" ]; then
      log ""
      log "============================================================"
      log "ERROR: /dev/shm too small for staging"
      log "============================================================"
      log ""
      log "Container size: ${source_size}MB"
      log "/dev/shm available: ${shm_avail}MB"
      log "/dev/shm is sized by the -m (memory) flag"
      log ""
      log "SOLUTION: Increase VM memory to at least ${shm_needed}m"
      log ""
      log "  docker run --runtime=runcvm -m ${shm_needed}m \\"
      log "    -e RUNCVM_HYPERVISOR=firecracker nginx"
      log ""
      log "============================================================"
      error "Insufficient /dev/shm for staging (need ${shm_needed}MB, have ${shm_avail}MB)"
    fi
    
    # Step 1: Copy container files to staging area
    log "Step 1/3: Copying container files to staging area..."
    log "  Source: $RUNCVM_VM_MOUNTPOINT (excluding .runcvm)"
    log "  Destination: $staging_dir"
    log "  This may take 30-60 seconds for large images..."
    
    busybox rm -rf "$staging_dir"
    busybox mkdir -p "$staging_dir"
    
    # Use tar to copy, excluding .runcvm directory
    local copy_start=$(busybox date +%s)
    
    if ! (cd "$RUNCVM_VM_MOUNTPOINT" && busybox tar -cf - --exclude='.runcvm' . 2>/dev/null | busybox tar -xf - -C "$staging_dir" 2>/dev/null); then
      error "Failed to copy container files to staging"
    fi
    
    local copy_end=$(busybox date +%s)
    local copy_time=$(( copy_end - copy_start ))
    
    local staged_size=$(busybox du -sm "$staging_dir" 2>/dev/null | busybox cut -f1)
    log "  Staging complete: ${staged_size}MB copied in ${copy_time}s"



   # Step 1b: Create init script in staging area
   log "  Creating init script..."
   local init_script="${staging_dir}/init"
   cat > "$init_script" << 'INITEOF'
#!/bin/sh
# Firecracker minimal init for RunCVM
# This runs as PID 1 inside the Firecracker VM

# ============================================================
# LOGGING SYSTEM (sh-compatible)
# ============================================================

export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

# Try to load RUNCVM_LOG_LEVEL from config if it exists
if [ -f /.runcvm/config ]; then
  # Extract RUNCVM_LOG_LEVEL from config (format: declare -x RUNCVM_LOG_LEVEL="VALUE")
  RUNCVM_LOG_LEVEL=$(grep '^declare -x RUNCVM_LOG_LEVEL=' /.runcvm/config 2>/dev/null | sed 's/^declare -x RUNCVM_LOG_LEVEL="\(.*\)"$/\1/' | head -1)
  
  # Extract RUNCVM_SYSTEMD
  RUNCVM_SYSTEMD=$(grep '^declare -x RUNCVM_SYSTEMD=' /.runcvm/config 2>/dev/null | sed 's/^declare -x RUNCVM_SYSTEMD="\(.*\)"$/\1/' | head -1)
fi

# Default to OFF if not set (matches host default for silent operation)
# NOTE: DSR terminal issue is fixed by having log output during boot; 
# users who want silent logs can set RUNCVM_LOG_LEVEL=OFF
RUNCVM_LOG_LEVEL="${RUNCVM_LOG_LEVEL:-OFF}"

# Logging function (simplified for sh)
# Logging function (simplified for sh)
log() {
  local severity="$1"
  shift
  local message="$*"
  
  # Always show ERRORs, otherwise check log level
  if [ "$severity" = "ERROR" ] || [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]; then
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] [RunCVM-FC-Init] [$severity] $message"
    return
  fi

  case "$RUNCVM_LOG_LEVEL" in
    INFO|LOG)
      case "$severity" in
        INFO) echo "[$(date '+%Y-%m-%d %H:%M:%S')] [RunCVM-FC-Init] [$severity] $message" ;;
      esac
      ;;
  esac
}

# Helper function to redirect output based on log level
# Usage: some_command $(output_redirect)
output_redirect() {
  if [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]; then
    echo ""  # No redirection
  else
    echo ">/dev/null 2>&1"
  fi
}

# Simpler helper - returns 0 if debug
is_debug() {
  [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]
}


log INFO "Starting..."

export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

# DEBUG: Check environment
if is_debug; then
  log INFO "Environment check:"
  log INFO "  PATH: $PATH"
  log INFO "  busybox: $(which busybox 2>/dev/null || echo 'not found')"
  if [ -x /bin/busybox ]; then
     log INFO "  /bin/busybox exists"
     log INFO "  cttyhack in busybox: $(/bin/busybox --list | grep cttyhack || echo 'no')"
  else
     log INFO "  /bin/busybox missing"
  fi
fi

# Mount essential filesystems
mount -t proc proc /proc 2>/dev/null || true
mount -t sysfs sys /sys 2>/dev/null || true
mount -t devtmpfs dev /dev 2>/dev/null || true

# Create essential device nodes if devtmpfs failed
if [ ! -c /dev/null ]; then
  mknod -m 666 /dev/null c 1 3
  mknod -m 666 /dev/zero c 1 5
  mknod -m 666 /dev/random c 1 8
  mknod -m 666 /dev/urandom c 1 9
  mknod -m 666 /dev/tty c 5 0
  mknod -m 620 /dev/console c 5 1
  mknod -m 666 /dev/ptmx c 5 2
fi

# Mount pts for proper terminal support
mkdir -p /dev/pts /dev/shm
mount -t devpts devpts /dev/pts 2>/dev/null || true
mount -t tmpfs tmpfs /dev/shm 2>/dev/null || true

# Create tmpfs for /run and /tmp
mkdir -p /run /tmp
mount -t tmpfs tmpfs /run 2>/dev/null || true
# Create tmpfs for /run and /tmp
mkdir -p /run /tmp
mount -t tmpfs tmpfs /run -o mode=0755,nosuid,nodev 2>/dev/null || true
mount -t tmpfs tmpfs /tmp -o mode=1777,strictatime,nosuid,nodev 2>/dev/null || true
mkdir -p /run/lock

# Mount cgroup v2 (Systemd requirement)
mkdir -p /sys/fs/cgroup
mount -t cgroup2 cgroup2 /sys/fs/cgroup 2>/dev/null || true

# Create symlinks
# Create symlinks
ln -sf /proc/self/fd /dev/fd 2>/dev/null || true
ln -sf /proc/self/fd/0 /dev/stdin 2>/dev/null || true
ln -sf /proc/self/fd/1 /dev/stdout 2>/dev/null || true
ln -sf /proc/self/fd/2 /dev/stderr 2>/dev/null || true

# Force all output to console (now that devices exist)
exec >/dev/console 2>&1

# Setup hostname
[ -f /etc/hostname ] && hostname -F /etc/hostname 2>/dev/null || true

# Setup networking
log INFO "========== NETWORK SETUP START =========="

# Setup RunCVM tools if available
# These tools use BundELF and need the dynamic linker from the same directory
# IMPORTANT: Path must match the original /.runcvm/guest/ because of relative RPATH
RUNCVM_GUEST="/.runcvm/guest"
if [ -d "$RUNCVM_GUEST/lib" ]; then
  # The dynamic linker is at /.runcvm/guest/lib/ld
  RUNCVM_LD="$RUNCVM_GUEST/lib/ld"
  if [ -x "$RUNCVM_LD" ]; then
    log INFO "Found RunCVM tools at $RUNCVM_GUEST"
    log INFO "Checking tools structure:"
    log DEBUG "lib/ld: $(ls -la $RUNCVM_GUEST/lib/ld 2>&1)"
    log DEBUG "bin contents: $(ls -la $RUNCVM_GUEST/bin/ 2>&1 | head -10)"
    log DEBUG "bin/ip: $(ls -la $RUNCVM_GUEST/bin/ip 2>&1)"
    log DEBUG "bin/busybox: $(ls -la $RUNCVM_GUEST/bin/busybox 2>&1)"
    
    # Test the dynamic linker directly
    log INFO "Testing dynamic linker..."
    log DEBUG "Test 1 - ld exists: $(test -x $RUNCVM_LD && echo yes || echo no)"
    log DEBUG "Test 2 - busybox via ld: $($RUNCVM_LD $RUNCVM_GUEST/bin/busybox echo 'works' 2>&1)"
    
    # ip is likely a symlink to busybox, so we need to call busybox ip
    # Create wrapper functions that use the dynamic linker with busybox
    runcvm_ip() { "$RUNCVM_LD" "$RUNCVM_GUEST/bin/busybox" ip "$@"; }
    runcvm_busybox() { "$RUNCVM_LD" "$RUNCVM_GUEST/bin/busybox" "$@"; }
    HAVE_RUNCVM_TOOLS=1
  else
    log INFO "Dynamic linker not found at $RUNCVM_LD"
    # Fallback: check if tools work anyway (e.g. static busybox)
    if "$RUNCVM_GUEST/bin/busybox" true 2>/dev/null; then
       log INFO "Busybox works without explicit LD check"
       runcvm_ip() { "$RUNCVM_GUEST/bin/busybox" ip "$@"; }
       runcvm_busybox() { "$RUNCVM_GUEST/bin/busybox" "$@"; }
       HAVE_RUNCVM_TOOLS=1
    fi
  fi
else
  log INFO "RunCVM tools not found at $RUNCVM_GUEST"
fi

# Bring up loopback
if [ "$HAVE_RUNCVM_TOOLS" = "1" ]; then
  runcvm_ip link set lo up 2>/dev/null  # Ensure loopback is up
  ip link set lo up 2>/dev/null || ifconfig lo up 2>/dev/null || true
  
  # Configure eth0 with static IP
  # Cloud-init usually handles this, but for non-cloud-init images (or early boot connectivity)
  # we set it up.
  # CRITICAL: We also add a route to 169.254.1.1 (the bridge IP in the container namespace)
  # This allows 'docker exec' (runcvm-ctr-exec) to receive return traffic from the VM.
  # Otherwise, VM replies to default GW (172.17.0.1) which drops 169.254.x.x traffic.
  
  # Wait for interface
  echo "Waiting for eth0..."
  for i in $(seq 1 50); do
    if ip link show eth0 >/dev/null 2>&1; then
      break
    fi
    sleep 0.1
  done
  
  # Set IP if variables passed
  # The variables RUNCVM_IP etc are injected by runcvm-ctr-firecracker into init script
  # We should ensure they are written to the init script.
  # Currently runcvm-ctr-firecracker does NOT write specific IP vars to init script directly,
  # it relies on cloud-init or network config files.
  # BUT we can add the route using 'ip' command if 'ip' is available.
  
  if command -v ip >/dev/null 2>&1; then
     ip link set eth0 up 2>/dev/null || true
     ip link set eth0 up 2>/dev/null || true
  fi
  
  # Flush any potential firewall rules that might block SSH
  if command -v iptables >/dev/null 2>&1; then
     iptables -F 2>/dev/null || true
     iptables -P INPUT ACCEPT 2>/dev/null || true
  fi
fi

# First, check what kernel modules are loaded for networking
log INFO "Checking for virtio_net module..."
if is_debug && [ -f /proc/modules ]; then
  grep -i virtio /proc/modules 2>/dev/null || echo "  No virtio modules loaded"
fi

# Check /sys/class/net to see what the kernel sees
log INFO "Kernel network interfaces in /sys/class/net:"
if is_debug; then
  ls -la /sys/class/net/ 2>/dev/null || echo "  Cannot list /sys/class/net"
fi

# Check dmesg for network-related messages
log INFO "Recent dmesg network messages:"
if is_debug; then
  dmesg 2>/dev/null | grep -iE "(eth|net|virtio)" | tail -10 || echo "  Cannot read dmesg"
fi

# Configure all network interfaces from config files or DHCP
log INFO "Configuring network..."

# We need to find all eth* interfaces
# Since we might not have 'ls' or 'find' behaving standardly, we look at sysfs
if is_debug; then
  ls -la /sys/class/net/
fi

# Iterate over eth interfaces found in sysfs
found_ifaces=0
for iface_path in /sys/class/net/eth*; do
  # Check if glob expansion failed
  [ -e "$iface_path" ] || continue
  
  IFACE=$(basename "$iface_path")
  found_ifaces=1
  log INFO "Configuring interface $IFACE..."

  # Look for config file
  CONFIG_FILE="/.runcvm-network-${IFACE}"
  
  # Backward compat: check legacy file for eth0 if new one missing
  if [ "$IFACE" = "eth0" ] && [ ! -f "$CONFIG_FILE" ] && [ -f "/.runcvm-network" ]; then
     CONFIG_FILE="/.runcvm-network"
  fi
  
  if [ -f "$CONFIG_FILE" ]; then
    log INFO "  Loading config from $CONFIG_FILE"
    if is_debug; then cat "$CONFIG_FILE"; fi
    
    # unset previous vars to be safe
    unset FC_IP FC_PREFIX FC_GW FC_MTU FC_MAC
    . "$CONFIG_FILE"
    
    if [ -n "$FC_IP" ] && [ -n "$FC_PREFIX" ]; then
       log INFO "  Setting IP $FC_IP/$FC_PREFIX (MTU: ${FC_MTU:-1500})"
       
       # Determine tool to use
       if [ "$HAVE_RUNCVM_TOOLS" = "1" ]; then
         IP_CMD="runcvm_ip"
       elif command -v ip >/dev/null 2>&1; then
         IP_CMD="ip"
       elif command -v ifconfig >/dev/null 2>&1; then
         IP_CMD=""
         USE_IFCONFIG=1
       else
         IP_CMD=""
       fi
       
       if [ -n "$IP_CMD" ]; then
          $IP_CMD link set "$IFACE" up 2>/dev/null || true
          [ -n "$FC_MTU" ] && $IP_CMD link set "$IFACE" mtu "$FC_MTU" 2>/dev/null || true
          $IP_CMD addr add "$FC_IP/$FC_PREFIX" dev "$IFACE"
          
          if [ -n "$FC_GW" ] && [ "$FC_GW" != "-" ]; then
             log INFO "  Adding default gateway $FC_GW"
             $IP_CMD route add default via "$FC_GW" dev "$IFACE"
             
             # If this is the default GW interface, add the bridge route for 9P too
             # (This is mostly relevant for the primary interface)
             $IP_CMD route add 169.254.1.1/32 dev "$IFACE" 2>/dev/null || true
          fi
          
       elif [ "$USE_IFCONFIG" = "1" ]; then
          # Basic ifconfig support
          case "$FC_PREFIX" in
            8)  FC_NETMASK="255.0.0.0" ;;
            16) FC_NETMASK="255.255.0.0" ;;
            24) FC_NETMASK="255.255.255.0" ;;
            *)  FC_NETMASK="255.255.255.0" ;;
          esac
          ifconfig "$IFACE" "$FC_IP" netmask "$FC_NETMASK" up
          if [ -n "$FC_GW" ] && [ "$FC_GW" != "-" ]; then
             route add default gw "$FC_GW" 2>/dev/null || true
          fi
       fi
    else
       log ERROR "  Config file found but missing IP/Prefix!"
    fi
  else
    # Fallback to DHCP if no config
    log INFO "  No config file, trying DHCP..."
    ip link set "$IFACE" up 2>/dev/null || ifconfig "$IFACE" up 2>/dev/null || true
    if command -v udhcpc >/dev/null 2>&1; then
      udhcpc -i "$IFACE" -n -q 2>/dev/null || true
    fi
  fi
done

if [ "$found_ifaces" = "0" ]; then
  log INFO "No ethernet interfaces found!"
fi

log INFO "========== NETWORK SETUP END =========="

# Setup DNS
if [ -f /.runcvm-resolv.conf ]; then
  cp /.runcvm-resolv.conf /etc/resolv.conf 2>/dev/null || true
fi

# ==========================================================================
# NFS VOLUME MOUNTS - Mount NFS shares from host unfsd
# ==========================================================================
mount_nfs_volumes() {
  local nfs_config="/.runcvm/nfs-mounts"
  
  # Get gateway IP from network config (this is the HOST from VM's perspective)
  # Get gateway IP from network config (this is the HOST from VM's perspective)
  local host_ip=""
  
  # Scan all network configs for a gateway
  # We prioritize eth0 if it exists
  if [ -f "/.runcvm-network-eth0" ]; then
     unset FC_GW
     . "/.runcvm-network-eth0"
     if [ -n "$FC_GW" ] && [ "$FC_GW" != "-" ]; then
       host_ip="$FC_GW"
     fi
  fi
  
  # If not found in eth0, try others
  if [ -z "$host_ip" ]; then
    for cfg in /.runcvm-network-*; do
      [ -f "$cfg" ] || continue
      unset FC_GW
      . "$cfg"
      if [ -n "$FC_GW" ] && [ "$FC_GW" != "-" ]; then
        host_ip="$FC_GW"
        break
      fi
    done
  fi
  
  # Fallback to default Docker gateway if still nothing
  if [ -z "$host_ip" ] || [ "$host_ip" = "-" ]; then
    log INFO "Warning: No gateway found in network configs, defaulting to 172.17.0.1"
    host_ip="172.17.0.1"
  fi
  
  log INFO "Checking for NFS volumes..."
  log INFO "  Host IP (for NFS): $host_ip"
  
  if [ ! -f "$nfs_config" ]; then
    log INFO "No nfs-mounts config found - volumes are static copies"
    return 0
  fi
  
  log INFO "NFS Transport: TCP over $host_ip"
  log INFO "Mount type: NFS (live, bidirectional)"
  
  # Config format: src:dst:port
  # Mount each volume via NFS
  while IFS=: read -r src_path dst nfs_port; do
    [ -z "$src_path" ] && continue
    [ -z "$dst" ] && continue
    
    log INFO "  Mounting $src_path -> $dst (port $nfs_port)..."
    mkdir -p "$dst"
    
    # Mount via NFS v3 with nolock (no separate lockd needed)
    mount -t nfs -o vers=3,nolock,tcp,port="$nfs_port",mountport="$((nfs_port + 1))" \
      "$host_ip:$src_path" "$dst" 2>&1 | sed 's/^/    /'
    
    if mount | grep -q "$dst"; then
      log INFO "  ✓ Successfully mounted $dst (NFS)"
    else
      log ERROR "  ✗ Failed to mount $dst via NFS"
      log INFO "    Falling back to static copy mode"
    fi
  done < "$nfs_config"
  
  log INFO "  NFS mounts complete"
}

log INFO "========== NFS VOLUME MOUNTS =========="
mount_nfs_volumes
log INFO "========== NFS VOLUME MOUNTS END =========="

# ========== DROPBEAR SSH SERVER FOR DOCKER EXEC ==========
# docker exec uses SSH to connect to the VM, so we need dropbear running
log INFO "========== DROPBEAR SSH SETUP =========="

SSHD_PORT=22222
DROPBEAR_DIR="/.runcvm/dropbear"

if [ "$HAVE_RUNCVM_TOOLS" = "1" ]; then
  mkdir -p "$DROPBEAR_DIR"
  
  # Check if we need to generate keys
  if [ ! -f "$DROPBEAR_DIR/key" ]; then
    log INFO "Generating dropbear SSH keys..."
    # Generate ed25519 key
    "$RUNCVM_LD" "$RUNCVM_GUEST/usr/bin/dropbearkey" -t ed25519 -f "$DROPBEAR_DIR/key" >/dev/null 2>&1
    
    # Extract public key for EPKA
    KEY_PUBLIC=$("$RUNCVM_LD" "$RUNCVM_GUEST/usr/bin/dropbearkey" -y -f "$DROPBEAR_DIR/key" 2>/dev/null | grep ^ssh | cut -d' ' -f2)
    
    # Create EPKA config
    cat > "$DROPBEAR_DIR/epka.json" << EPKAEOF
[{"user":"root","keytype":"ssh-ed25519","key":"$KEY_PUBLIC","options":"no-X11-forwarding","comments":""}]
EPKAEOF
    chmod 400 "$DROPBEAR_DIR/epka.json" "$DROPBEAR_DIR/key"
    log INFO "SSH keys generated"
    log INFO "DEBUG: Keys in $DROPBEAR_DIR:"
    ls -la "$DROPBEAR_DIR" | while read line; do log INFO "  $line"; done
  else
    log INFO "Using pre-generated SSH keys"
    log INFO "DEBUG: Pre-existing keys in $DROPBEAR_DIR:"
    ls -la "$DROPBEAR_DIR" | while read line; do log INFO "  $line"; done
  fi
  
  # Start dropbear SSH server
  # Only start manually if NOT running Systemd (Systemd service handles it otherwise)
  if [ "$RUNCVM_SYSTEMD" != "1" ] && [ "$RUNCVM_SYSTEMD" != "true" ]; then
    log INFO "Starting dropbear on port $SSHD_PORT..."
  EPKA_LIB="$RUNCVM_GUEST/tmp/dropbear/libepka_file.so"
  
  if [ -f "$EPKA_LIB" ]; then
    "$RUNCVM_LD" "$RUNCVM_GUEST/usr/sbin/dropbear" -REF -p "$SSHD_PORT" \
      -A "$EPKA_LIB,$DROPBEAR_DIR/epka.json" \
      -P "$DROPBEAR_DIR/dropbear.pid" 2>/dev/null &
    sleep 0.5
    if [ -f "$DROPBEAR_DIR/dropbear.pid" ]; then
      log INFO "Dropbear started (PID: $(cat $DROPBEAR_DIR/dropbear.pid))"
    else
      log INFO "Warning - Dropbear may not have started correctly"
    fi
  else
      log INFO "Warning - EPKA library not found at $EPKA_LIB"
      log INFO "docker exec may not work"
    fi
  else
    log INFO "Systemd mode detected - skipping manual dropbear start (handled by service)"
  fi
else
  log INFO "RunCVM tools not available, skipping dropbear"
  log INFO "docker exec will not work"
fi

log INFO "========== DROPBEAR SETUP END =========="

# Note: Static watch binary (from procps) is copied to /usr/bin/watch during rootfs staging
# This provides proper Ctrl-C handling unlike busybox watch

# ========== TTY AND SIGNAL SETUP ==========
# This is critical for proper signal handling (Ctrl-C, Ctrl-Z, etc.)
# Without this, interactive commands like 'watch' won't respond to signals.
#
# IMPORTANT: We cannot use setsid with exec because:
# - setsid forks, parent exits immediately -> kernel panic (PID 1 cannot exit)
#
# Instead, we:
# 1. Redirect I/O to serial console for proper terminal
# 2. Run entrypoint as a child process (not exec)
# 3. Set up trap to forward signals to child
# 4. Wait for child to finish
# 5. Trigger proper shutdown (reboot -f) - PID 1 must never exit normally

# Determine what to run
# Priority:
# 1. /.runcvm-entrypoint (saved by RunCVM)
# 2. /docker-entrypoint.sh (nginx and many others)
# 3. Direct nginx execution
# 4. /bin/sh fallback

# TTY Handling Strategy:
# 1. cttyhack (Busybox) - Best, designed for this exactly
# 2. setsid (util-linux/Busybox) - Good, creates new session. 
#    Need to explicitly open /dev/console to make it the controlling terminal.

CTTYHACK=""
if command -v cttyhack >/dev/null 2>&1; then
  CTTYHACK="cttyhack"
elif command -v busybox >/dev/null 2>&1 && busybox --list | grep -q cttyhack; then
  CTTYHACK="busybox cttyhack"
elif [ -x "/bin/busybox" ] && /bin/busybox --list | grep -q cttyhack; then
  CTTYHACK="/bin/busybox cttyhack"
elif [ -x "/.runcvm/guest/bin/busybox" ] && /.runcvm/guest/bin/busybox --list | grep -q cttyhack; then
  if [ -x "/.runcvm/guest/lib/ld" ]; then
    CTTYHACK="/.runcvm/guest/lib/ld /.runcvm/guest/bin/busybox cttyhack"
  else
    CTTYHACK="/.runcvm/guest/bin/busybox cttyhack"
  fi
fi

if [ -n "$CTTYHACK" ]; then
  # Option 1: cttyhack found
  if [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]; then
    echo "RunCVM-FC init: TTY enabled ($CTTYHACK)"
  fi
  
  run_with_tty() {
    # Run as child so we can reboot after
    $CTTYHACK "$@"
    return $?
  }
elif command -v setsid >/dev/null 2>&1; then
  # Option 2: setsid found
  if [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]; then
    echo "RunCVM-FC init: TTY enabled (setsid fallback)"
  fi
  
  # Wrapper to run in new session and acquire controlling terminal
  # We exec a shell that opens console (becoming ctty) and then execs the target
  # We do NOT exec the setid command itself, so PID 1 stays alive to reboot
  # Note: setsid -c sets the controlling terminal to stdin (which we redirect/open properly)
  run_with_tty() {
    setsid -c sh -c 'exec "$@" <> /dev/ttyS0 >&0 2>&1' -- "$@"
    return $?
  }
else
  # Option 3: No TTY tools
  echo "RunCVM-FC init: WARNING - No TTY tools found (cttyhack/setsid)"
  echo "RunCVM-FC init: Interactive shells may not work correctly"
  
  run_with_tty() {
    "$@"
    return $?
  }
fi

# Check for Systemd
SYSTEMD_BIN=""
if [ -x /usr/lib/systemd/systemd ]; then
  SYSTEMD_BIN="/usr/lib/systemd/systemd"
elif [ -x /lib/systemd/systemd ]; then
  SYSTEMD_BIN="/lib/systemd/systemd"
elif [ -x /sbin/init ] && [ "$(/sbin/init --version 2>/dev/null | grep -c systemd)" -gt 0 ]; then
  SYSTEMD_BIN="/sbin/init"
fi

# Detect if we should run systemd
# Priority:
# 1. RUNCVM_SYSTEMD=1 env var
# 2. image has systemd installed (and we decide to auto-enable? No, safer to require flag for now or just check if entrypoint is empty/default)

if is_debug; then
  log DEBUG "Checking for Systemd..."
  log DEBUG "  Detailed check:"
  ls -la /usr/lib/systemd/systemd 2>/dev/null || log DEBUG "  /usr/lib/systemd/systemd not found"
  ls -la /lib/systemd/systemd 2>/dev/null || log DEBUG "  /lib/systemd/systemd not found"
  ls -la /sbin/init 2>/dev/null || log DEBUG "  /sbin/init not found"
  log DEBUG "  PATH=$PATH"
  log DEBUG "  SYSTEMD_BIN='$SYSTEMD_BIN'"
  log DEBUG "  RUNCVM_SYSTEMD='$RUNCVM_SYSTEMD'"
fi

SHOULD_RUN_SYSTEMD=0
if [ "$RUNCVM_SYSTEMD" = "1" ] || [ "$RUNCVM_SYSTEMD" = "true" ]; then
  SHOULD_RUN_SYSTEMD=1
elif [ -n "$SYSTEMD_BIN" ] && [ ! -f /.runcvm-entrypoint ]; then
  # If no custom entrypoint and systemd exists, maybe?
  # Let's stick to explicit flag for now to avoid breaking existing generic containers
  :
fi
if is_debug; then
  log DEBUG "  SHOULD_RUN_SYSTEMD='$SHOULD_RUN_SYSTEMD'"
fi

if [ "$SHOULD_RUN_SYSTEMD" = "1" ] && [ -n "$SYSTEMD_BIN" ]; then
   echo "RunCVM-FC init: Booting with Systemd ($SYSTEMD_BIN)..."
   
   # Systemd requirements:
   # 1. PID 1 (we are)
   # 2. cgroup2 mounted (we did)
   # 3. /run mounted (we did)
   # 4. signal handling (systemd handles this)
   
   # Set container environment type
   export container=docker

   # Run systemd in a separate PID namespace so we (the init script) remain PID 1
   # This allows for better process management and "escaping"
   
   UNSHARE_BIN=""
   if command -v unshare >/dev/null 2>&1; then
      UNSHARE_BIN="unshare"
   elif command -v busybox >/dev/null 2>&1; then
      UNSHARE_BIN="busybox unshare"
   elif [ -x /bin/busybox ]; then
      UNSHARE_BIN="/bin/busybox unshare"
   fi
   
   if [ -n "$UNSHARE_BIN" ]; then
       echo "RunCVM-FC init: Starting Systemd in new PID namespace (using $UNSHARE_BIN)..."
       
       # Determine TTY handler for new namespace
       TTY_CMD=""
       if command -v setsid >/dev/null 2>&1; then
           TTY_CMD="setsid"
       elif command -v cttyhack >/dev/null 2>&1; then
           TTY_CMD="cttyhack" 
       fi

       # -f: Fork
       # -p: Unshare PID namespace
       # --mount-proc: Mount /proc for the new namespace
       # Use setsid if available to ensure systemd is session leader
       $UNSHARE_BIN -f -p --mount-proc $TTY_CMD "$SYSTEMD_BIN" --unit=multi-user.target
       
       RET=$?
       echo "RunCVM-FC init: Systemd exited with code $RET"
       busybox poweroff -f
   else
       echo "RunCVM-FC init: 'unshare' not found, falling back to exec (PID 1)"
       # Execute systemd - this REPLACES the init process
       exec "$SYSTEMD_BIN" --unit=multi-user.target
   fi
fi

if [ -f /.runcvm-entrypoint ] && [ -s /.runcvm-entrypoint ]; then
  # Read entrypoint line by line into an array-like structure
  set --
  while IFS= read -r line || [ -n "$line" ]; do
    set -- "$@" "$line"
  done < /.runcvm-entrypoint
  
  echo "RunCVM-FC init: Running saved entrypoint: $@"
  run_with_tty "$@"
  
elif [ -x /docker-entrypoint.sh ]; then
  echo "RunCVM-FC init: Running /docker-entrypoint.sh"
  if [ -f /etc/nginx/nginx.conf ]; then
    run_with_tty /docker-entrypoint.sh nginx -g "daemon off;"
  else
    run_with_tty /docker-entrypoint.sh
  fi
  
elif [ -f /etc/nginx/nginx.conf ] && command -v nginx >/dev/null 2>&1; then
  echo "RunCVM-FC init: Running nginx directly"
  run_with_tty nginx -g "daemon off;"
  
else
  echo "RunCVM-FC init: No entrypoint found, starting shell"
  run_with_tty /bin/sh
fi

# We should only get here if run_with_tty failed to exec (e.g. command not found)
# OR if run_with_tty was not an exec (which it is currently)
RET=$?
echo "RunCVM-FC init: Entrypoint exited with code $RET"
busybox poweroff -f
INITEOF
  busybox chmod +x "$init_script"
  
  # Save the original entrypoint if we have it
  if [ -f "$entrypoint_file" ]; then
    busybox cp "$entrypoint_file" "${staging_dir}/.runcvm-entrypoint"
    log "  Saved entrypoint: $(busybox head -1 ${staging_dir}/.runcvm-entrypoint)"
  fi
  
  # Copy essential networking tools from RunCVM guest
  # These are needed because minimal images (like nginx) don't have ip/ifconfig
  # IMPORTANT: We must copy to the SAME path (/.runcvm/guest) because the binaries
  # use relative RPATH that depends on the directory structure
  log "  Copying network tools to rootfs..."
  
  if [ -d "$RUNCVM_GUEST" ]; then
    # Create the same directory structure
    busybox mkdir -p "${staging_dir}/.runcvm/guest"
    
    # Copy the dynamic linker and libraries
    if [ -d "$RUNCVM_GUEST/lib" ]; then
      busybox cp -a "$RUNCVM_GUEST/lib" "${staging_dir}/.runcvm/guest/"
      log "    Copied lib/ (dynamic linker)"
    fi
    
    # Copy usr/lib for additional libraries
    if [ -d "$RUNCVM_GUEST/usr/lib" ]; then
      busybox mkdir -p "${staging_dir}/.runcvm/guest/usr"
      busybox cp -a "$RUNCVM_GUEST/usr/lib" "${staging_dir}/.runcvm/guest/usr/"
      log "    Copied usr/lib/"
    fi
    
    # Copy bin directory with busybox and symlinks
    if [ -d "$RUNCVM_GUEST/bin" ]; then
      busybox cp -a "$RUNCVM_GUEST/bin" "${staging_dir}/.runcvm/guest/"
      log "    Copied bin/ (busybox, ip, etc.)"
    fi
    
    # Copy sbin if exists
    if [ -d "$RUNCVM_GUEST/sbin" ]; then
      busybox cp -a "$RUNCVM_GUEST/sbin" "${staging_dir}/.runcvm/guest/"
      log "    Copied sbin/"
    fi
    
    # Copy usr/bin and usr/sbin
    if [ -d "$RUNCVM_GUEST/usr/bin" ]; then
      busybox mkdir -p "${staging_dir}/.runcvm/guest/usr"
      busybox cp -a "$RUNCVM_GUEST/usr/bin" "${staging_dir}/.runcvm/guest/usr/"
      log "    Copied usr/bin/"
    fi
    if [ -d "$RUNCVM_GUEST/usr/sbin" ]; then
      busybox mkdir -p "${staging_dir}/.runcvm/guest/usr"
      busybox cp -a "$RUNCVM_GUEST/usr/sbin" "${staging_dir}/.runcvm/guest/usr/"
      log "    Copied usr/sbin/"
    fi
    
    # Copy usr/share (contains terminfo for ncurses programs like watch)
    if [ -d "$RUNCVM_GUEST/usr/share" ]; then
      busybox mkdir -p "${staging_dir}/.runcvm/guest/usr"
      busybox cp -a "$RUNCVM_GUEST/usr/share" "${staging_dir}/.runcvm/guest/usr/"
      log "    Copied usr/share/ (terminfo)"
    fi
    
    # Copy tmp/dropbear (contains EPKA library for SSH authentication)
    if [ -d "$RUNCVM_GUEST/tmp/dropbear" ]; then
      busybox mkdir -p "${staging_dir}/.runcvm/guest/tmp"
      busybox cp -a "$RUNCVM_GUEST/tmp/dropbear" "${staging_dir}/.runcvm/guest/tmp/"
      log "    Copied tmp/dropbear/ (EPKA library)"
    fi
    
    # Copy scripts directory (needed for runcvm-vm-exec)
    if [ -d "$RUNCVM_GUEST/scripts" ]; then
      busybox cp -a "$RUNCVM_GUEST/scripts" "${staging_dir}/.runcvm/guest/"
      log "    Copied scripts/"
    fi
  else
    log "  WARNING: RUNCVM_GUEST not found, network tools may not work"
  fi
  
  # Install procps watch wrapper (overrides busybox watch)
  # This provides proper Ctrl-C handling
  # The bundled watch uses BUNDELF and needs the dynamic linker at /.runcvm/guest/lib/ld
  log "  Checking for static watch binary..."
  
  WATCH_BIN=""
  for path in "/.runcvm/guest/bin/watch" "/.runcvm/bin/watch" "/opt/runcvm/bin/watch"; do
    if [ -f "$path" ]; then
      WATCH_BIN="$path"
      break
    fi
  done
  
  if [ -n "$WATCH_BIN" ]; then
    # Remove busybox symlink first (it's at /bin/watch)
    busybox rm -f "${staging_dir}/bin/watch" 2>/dev/null || true
    busybox rm -f "${staging_dir}/usr/bin/watch" 2>/dev/null || true
    
    # Create wrapper script that uses BUNDELF dynamic linker and terminfo
    busybox mkdir -p "${staging_dir}/bin"
    busybox cat > "${staging_dir}/bin/watch" << 'WATCHEOF'
#!/bin/sh
# Wrapper for procps watch (uses BUNDELF dynamic linker)
# Provides proper Ctrl-C handling unlike busybox watch
export TERMINFO=/.runcvm/guest/usr/share/terminfo
exec /.runcvm/guest/lib/ld /.runcvm/guest/bin/watch "$@"
WATCHEOF
    busybox chmod +x "${staging_dir}/bin/watch"
    log "    ✓ Installed procps watch wrapper to /bin/watch"
  else
    log "    ✗ Bundled watch not found"
  fi
  
  # Copy pre-generated dropbear keys from container (generated by runcvm-ctr-entrypoint)
  if [ -d "/.runcvm/dropbear" ] && [ -f "/.runcvm/dropbear/key" ]; then
    busybox mkdir -p "${staging_dir}/.runcvm/dropbear"
    busybox cp -a "/.runcvm/dropbear/"* "${staging_dir}/.runcvm/dropbear/"
    log "    Copied pre-generated dropbear keys"
  fi
  
  # Copy the config file (needed by some scripts)
  if [ -f "/.runcvm/config" ]; then
    busybox cp "/.runcvm/config" "${staging_dir}/.runcvm/config"
    log "    Copied /.runcvm/config"
  fi
  # Copy nfs-mounts file - CRITICAL for NFS volume mounting in VM
  log_debug "Checking NFS_CONFIG at $NFS_CONFIG"
  if [ -f "$NFS_CONFIG" ]; then
    log_debug "NFS_CONFIG exists, copying to staging..."
    if [ -s "$NFS_CONFIG" ]; then
      busybox cp "$NFS_CONFIG" "${staging_dir}/.runcvm/nfs-mounts"
      log_debug "Copied to ${staging_dir}/.runcvm/nfs-mounts"
      log_debug "Contents:"
      busybox cat "${staging_dir}/.runcvm/nfs-mounts" 2>&1 | while read line; do log_debug "  $line"; done
    else
      log "WARNING: NFS_CONFIG exists but is empty"
    fi
  else
    log "WARNING: NFS_CONFIG does not exist at $NFS_CONFIG"
  fi

  # Copy kernel modules for 9P support from Alpine kernel

  log "  Copying kernel modules..."
  local fc_modules_dir="$RUNCVM_GUEST/kernels/firecracker/latest/modules"
  if [ -d "$fc_modules_dir" ]; then
    # Find the kernel version directory
    local kernel_ver=$(busybox ls "$fc_modules_dir" 2>/dev/null | busybox head -1)
    if [ -n "$kernel_ver" ] && [ -d "$fc_modules_dir/$kernel_ver" ]; then
      log "    Found kernel modules for version: $kernel_ver"
      
      # Create modules directory in staging
      busybox mkdir -p "${staging_dir}/lib/modules/$kernel_ver"
      # Copy vsock modules if they exist
      if [ -d "$fc_modules_dir/$kernel_ver/kernel/net/vmw_vsock" ]; then
        busybox mkdir -p "${staging_dir}/lib/modules/$kernel_ver/kernel/net"
        busybox cp -a "$fc_modules_dir/$kernel_ver/kernel/net/vmw_vsock" \
          "${staging_dir}/lib/modules/$kernel_ver/kernel/net/"
        log "      Copied vsock modules"
      fi
      
      # Copy modules.* files (needed for modprobe)
      busybox cp "$fc_modules_dir/$kernel_ver"/modules.* \
        "${staging_dir}/lib/modules/$kernel_ver/" 2>/dev/null || true
      log "      Copied module metadata files"
    else
      log "    WARNING: No kernel modules found in $fc_modules_dir"
    fi
  else
    log "    WARNING: Firecracker modules directory not found: $fc_modules_dir"
  fi

  
  # Create a simpler runcvm-vm-exec for Firecracker that doesn't depend on complex paths
  # This is called by dropbear when docker exec connects via SSH
  busybox cat > "${staging_dir}/.runcvm/guest/scripts/runcvm-vm-exec" << 'VMEXECEOF'
#!/bin/sh
# Simplified runcvm-vm-exec for Firecracker
# Called by dropbear SSH when docker exec connects
# Arguments: <uid:gid:groups> <cwd_encoded> <args_encoded> <env_encoded>

from_bin() {
  tr '\200\201\202\203\204\205' "\011\012\040\047\042\134"
}

uidgid="$1"
cwd_bin="$2"
args_bin="$3"
env_bin="$4"

# Decode working directory
cwd=$(printf '%s' "$cwd_bin" | from_bin)

# Change to working directory
cd "$cwd" 2>/dev/null || cd /

# Decode and execute the command
# The args are newline-separated after decoding
if [ -n "$args_bin" ]; then
  # Create a temporary script to handle the decoded args properly
  tmpscript="/tmp/exec-$$"
  printf '%s' "$args_bin" | from_bin > "$tmpscript.args"
  
  # Read args into positional parameters
  # We use a redirection loop which runs in current shell (usually) or use a FD
  # to avoid subshell if strictly POSIX but busybox sh handles this.
  
  # Reset positional parameters
  set --
  
  # Read the file line by line
  while IFS= read -r line || [ -n "$line" ]; do
    set -- "$@" "$line"
  done < "$tmpscript.args"
  
  rm -f "$tmpscript.args"
  
  # The first argument is the command (cmd)
  # But passing encoded args: <cmd> <arg1> <arg2>...
  # So $1 is cmd.
  
  # Execute the command
  exec "$@"

else
  # Single command fallback (should not happen with standard runcvm)
  # But if it does, assuming cmd comes from somewhere else?
  # The original logic parsed cmd from lines.
  # If args_bin is "cmd\narg1\narg2", then loop works.
  :
fi
  fi
else
  exec /bin/sh
fi
VMEXECEOF
  busybox chmod +x "${staging_dir}/.runcvm/guest/scripts/runcvm-vm-exec"
  log "    Created simplified runcvm-vm-exec"
  
  # Save network configuration for the VM
  # Save network configuration for all VMs
  if [ -d "/.runcvm/network/devices" ]; then
    for net_dev_file in $(busybox ls /.runcvm/network/devices/* | busybox sort); do
      local ifname=$(busybox basename "$net_dev_file")
      [ "$ifname" = "default" ] && continue
      
      read DOCKER_IF DOCKER_IF_MAC DOCKER_IF_MTU DOCKER_IF_IP DOCKER_IF_IP_NETPREFIX DOCKER_IF_IP_GW < "$net_dev_file"
      
      # Firecracker MAC
      local fc_mac=$(echo "$DOCKER_IF_MAC" | busybox sed 's/^..:..:../AA:FC:00/')
      
      log "  Saving network config for $ifname: IP=$DOCKER_IF_IP/$DOCKER_IF_IP_NETPREFIX GW=$DOCKER_IF_IP_GW"
      
      cat > "${staging_dir}/.runcvm-network-${ifname}" << NETEOF
FC_IP="$DOCKER_IF_IP"
FC_PREFIX="$DOCKER_IF_IP_NETPREFIX"
FC_GW="$DOCKER_IF_IP_GW"
FC_MTU="$DOCKER_IF_MTU"
FC_MAC="$fc_mac"
NETEOF
    done
  fi
  
  # Inject Systemd Service for Entrypoint
  if [ "$RUNCVM_SYSTEMD" = "1" ] || [ "$RUNCVM_SYSTEMD" = "true" ]; then
    log "  Systemd enabled: Injecting runcvm-entrypoint service..."
    
    # 1. Create the runner script (decodes /.runcvm-entrypoint and execs it)
    local runner_script="${staging_dir}/usr/local/bin/runcvm-entrypoint-runner"
    busybox mkdir -p "${staging_dir}/usr/local/bin"
    
    cat > "$runner_script" << 'RUNNEREOF'
#!/bin/sh
# Wrapper to run the container entrypoint under systemd
set -e

if [ -f /.runcvm-entrypoint ]; then
  # Read entrypoint line by line into arg array
  set --
  while IFS= read -r line || [ -n "$line" ]; do
    set -- "$@" "$line"
  done < /.runcvm-entrypoint
   
  echo "RunCVM: Starting entrypoint: $@"
  
  # Check if entrypoint is just init/systemd (which we are already running)
  # If so, do nothing (success)
  case "$1" in
    /sbin/init|/lib/systemd/systemd|/usr/lib/systemd/systemd)
      echo "RunCVM: Entrypoint is init/systemd - skipping recursive execution"
      exit 0
      ;;
    *)
      exec "$@"
      ;;
  esac
else
  echo "RunCVM: No entrypoint found"
  exit 0
fi
RUNNEREOF
    busybox chmod +x "$runner_script"
    
    # 2. Create the Systemd Unit
    local unit_file="${staging_dir}/etc/systemd/system/runcvm-entrypoint.service"
    busybox mkdir -p "${staging_dir}/etc/systemd/system"
    
    cat > "$unit_file" << 'UNITEOF'
[Unit]
Description=RunCVM Container Entrypoint
After=network.target cloud-init.service
Wants=network.target

[Service]
Type=exec
ExecStart=/usr/local/bin/runcvm-entrypoint-runner
StandardOutput=journal+console
StandardError=journal+console
Restart=no
RemainAfterExit=yes
# Make sure we have a TTY if needed (optional, but good for interactive)
# TTYPath=/dev/ttyS0
# TTYReset=yes
# TTYVHangup=yes

[Install]
WantedBy=multi-user.target
UNITEOF

    # 3. Enable the service (symlink)
    busybox mkdir -p "${staging_dir}/etc/systemd/system/multi-user.target.wants"
    # Use relative symlink (safer for chroot/offline enablement)
    busybox ln -sf "../runcvm-entrypoint.service" "${staging_dir}/etc/systemd/system/multi-user.target.wants/runcvm-entrypoint.service"
    
    # --------------------------------------------------------
    # Inject Dropbear Service (for docker exec support)
    # --------------------------------------------------------
    log "  Systemd enabled: Injecting runcvm-dropbear service (port 22222)..."
    
    # Create keygen helper
    local dropbear_keygen="${staging_dir}/usr/local/bin/runcvm-dropbear-keygen"
    cat > "$dropbear_keygen" << 'KEYGENEOF'
#!/bin/sh
set -e
RUNCVM_GUEST="/.runcvm/guest"
RUNCVM_LD="$RUNCVM_GUEST/lib/ld"
DROPBEAR_DIR="/.runcvm/dropbear"
mkdir -p "$DROPBEAR_DIR"

if [ ! -f "$DROPBEAR_DIR/key" ]; then
  "$RUNCVM_LD" "$RUNCVM_GUEST/usr/bin/dropbearkey" -t ed25519 -f "$DROPBEAR_DIR/key" >/dev/null 2>&1
fi

# Re-generate EPKA config every time to be safe
KEY_PUBLIC=$("$RUNCVM_LD" "$RUNCVM_GUEST/usr/bin/dropbearkey" -y -f "$DROPBEAR_DIR/key" 2>/dev/null | grep ^ssh | cut -d' ' -f2)
echo '[{"user":"root","keytype":"ssh-ed25519","key":"'"$KEY_PUBLIC"'","options":"no-X11-forwarding","comments":""}]' > "$DROPBEAR_DIR/epka.json"
chmod 400 "$DROPBEAR_DIR/epka.json" "$DROPBEAR_DIR/key"
KEYGENEOF
    busybox chmod +x "$dropbear_keygen"

    # Create start helper
    local dropbear_start="${staging_dir}/usr/local/bin/runcvm-dropbear-start"
    cat > "$dropbear_start" << 'STARTEOF'
#!/bin/sh
RUNCVM_GUEST="/.runcvm/guest"
RUNCVM_LD="$RUNCVM_GUEST/lib/ld"
DROPBEAR_DIR="/.runcvm/dropbear"
EPKA_LIB="$RUNCVM_GUEST/tmp/dropbear/libepka_file.so"
SSHD_PORT=22222

exec "$RUNCVM_LD" "$RUNCVM_GUEST/usr/sbin/dropbear" -REF -p "$SSHD_PORT" \
  -A "$EPKA_LIB,$DROPBEAR_DIR/epka.json"
STARTEOF
    busybox chmod +x "$dropbear_start"

    # Create Service Unit
    local dropbear_unit="${staging_dir}/etc/systemd/system/runcvm-dropbear.service"
    cat > "$dropbear_unit" << 'DBUNITEOF'
[Unit]
Description=RunCVM Dropbear SSH
After=network.target

[Service]
Type=simple
ExecStartPre=/usr/local/bin/runcvm-dropbear-keygen
ExecStart=/usr/local/bin/runcvm-dropbear-start
StandardOutput=journal+console
StandardError=journal+console
Restart=always

[Install]
WantedBy=multi-user.target
DBUNITEOF

    # Enable Dropbear
    # Use relative symlink
    busybox ln -sf "../runcvm-dropbear.service" "${staging_dir}/etc/systemd/system/multi-user.target.wants/runcvm-dropbear.service"
    
    # Disable conflicting OpenSSH on port 22 (optional, but good practice if we don't want it)
    # Actually, we keep it for user convenience, as dropbear is on 22222.
    # But if we want to save resources:
    # busybox rm -f "${staging_dir}/etc/systemd/system/sshd.service"
    # busybox rm -f "${staging_dir}/etc/systemd/system/ssh.service" 2>/dev/null || true
    # For now, leave OpenSSH enabled as users might want standard SSH access.
  fi

  # Save resolv.conf
  if [ -f "/etc/resolv.conf" ]; then
    busybox cp /etc/resolv.conf "${staging_dir}/.runcvm-resolv.conf"
  fi
  
  # Step 2: Calculate rootfs size (Minimal)
  # Just enough to hold files + small buffer (e.g. 100MB)
  # We will resize it LATER for the runtime instance.
  local rootfs_size=$(( staged_size + 150 ))
  [ "$rootfs_size" -lt 64 ] && rootfs_size=64
  
  log "Step 2/3: Creating rootfs image..."
  log "  Rootfs size: ${rootfs_size}MB (${staged_size}MB content + overhead)"
  
  # Check overlay disk space
  local overlay_avail=$(busybox df -m / 2>/dev/null | busybox awk 'NR==2 {print $4}')
  log "  Overlay (/) available: ${overlay_avail}MB"
  
  if [ "$overlay_avail" -lt "$rootfs_size" ]; then
    busybox rm -rf "$staging_dir"
    error "Insufficient disk space on overlay (need ${rootfs_size}MB, have ${overlay_avail}MB)"
  fi
  
  # Create rootfs from staging directory
  log "  Source: $staging_dir"  
  log "  Destination: $rootfs_path"
  
  local mke2fs_start=$(busybox date +%s)
  
  if ! create_rootfs_from_dir "$staging_dir" "$rootfs_path" "$rootfs_size"; then
    log "mke2fs debug info:"
    log "  Staged content size: $(busybox du -sm "$staging_dir" | busybox cut -f1)MB"
    log "  Image size: ${rootfs_size}MB"
    busybox rm -rf "$staging_dir"
    error "Failed to create rootfs"
  fi
  
  local mke2fs_end=$(busybox date +%s)
  local mke2fs_time=$(( mke2fs_end - mke2fs_start ))
  
  # Cleanup staging
  log "  Cleaning up staging area..."
  busybox rm -rf "$staging_dir"
  
  local final_size=$(busybox du -sm "$rootfs_path" 2>/dev/null | busybox cut -f1)
  log "  Rootfs ready: ${final_size}MB (created in ${mke2fs_time}s)"

  # CACHE UPDATE (Save SMALL image)
  if [ "$use_cache" = "1" ]; then
     log "Updating cache: $cache_file"
     if cp --sparse=always "$rootfs_path" "$cache_file" 2>/dev/null; then
        :
     else
        busybox cp "$rootfs_path" "$cache_file"
     fi
  fi
  
  # Expand rootfs for Runtime
  TARGET_SIZE="${RUNCVM_ROOTFS_SIZE:-256M}"
  log "Resizing rootfs to $TARGET_SIZE..."
  if command -v truncate >/dev/null 2>&1; then
     truncate -s "$TARGET_SIZE" "$rootfs_path"
  else
     local size_bytes=$(( 2 * 1024 * 1024 * 1024 ))
     busybox dd of="$rootfs_path" bs=1 count=0 seek="$size_bytes" 2>/dev/null || true
  fi
  if command -v resize2fs >/dev/null 2>&1; then
     resize2fs "$rootfs_path" >/dev/null 2>&1
  fi

fi # End Cache Miss/No Cache logic

  
  # Determine kernel path - using Firecracker kernel Image (uncompressed)
  # The kernel is copied to /opt/runcvm/kernels/firecracker/vmlinux in Dockerfile
  # which becomes /.runcvm/guest/kernels/firecracker/vmlinux at runtime
  local fc_default_kernel="$RUNCVM_GUEST/kernels/firecracker/vmlinux"
  local kernel_path="${RUNCVM_FC_KERNEL_PATH:-$fc_default_kernel}"
  
  if [ ! -f "$kernel_path" ]; then
    error "Firecracker kernel not found: $kernel_path"
  fi
  
  # DEBUG: Show kernel file details
  log "DEBUG: Kernel file information:"
  log "  Path: $kernel_path"
  log "  Size: $(busybox ls -lh "$kernel_path" 2>/dev/null | busybox awk '{print $5}')"
  log "  Modified: $(busybox ls -l "$kernel_path" 2>/dev/null | busybox awk '{print $6, $7, $8}')"
  if command -v sha256sum >/dev/null 2>&1; then
    log "  SHA256: $(sha256sum "$kernel_path" 2>/dev/null | busybox cut -d' ' -f1)"
  fi
  
  # Build boot arguments - use /init which we created in the rootfs
  # Note: 9p.debug=0xff enables verbose 9P logging to debug transport issues
  local boot_args="init=/init console=ttyS0 reboot=k panic=1 pci=off root=/dev/vda rw"
  if [ -n "$RUNCVM_KERNEL_ARGS" ]; then
      boot_args="$boot_args $RUNCVM_KERNEL_ARGS"
  fi
  # Add 'quiet' to suppress kernel boot messages when not in DEBUG mode
  if [ "$RUNCVM_LOG_LEVEL" != "DEBUG" ]; then
    boot_args="$boot_args quiet loglevel=0 systemd.show_status=false"
  fi

  
  # Parse memory size (remove 'M' suffix if present)
  local mem_mb="${RUNCVM_MEM_SIZE%M}"
  [ -z "$mem_mb" ] && mem_mb=768
  
  # Parse CPU count
  local vcpu_count="${RUNCVM_CPUS:-1}"
  [ "$vcpu_count" -le 0 ] && vcpu_count=$(busybox nproc)
  
  log "Step 3/3: Starting Firecracker VM..."
  log "  Kernel: $kernel_path"
  log "  Config: $vcpu_count vCPUs, ${mem_mb}MB RAM"
  
  # Note: /.runcvm keys are copied into rootfs during creation (see earlier COPY steps)
  # Firecracker does not support bind mounts, so we rely on copying or NFS.

  
  # Build network config if available
  local network_interfaces_json=""
  local is_host_mode=0
  [ -f "/.runcvm/network/host_mode" ] && is_host_mode=1
  
  # Iterate over all defined network devices
  # We use a sorted list to ensure eth0 comes first (important for default gateway)
  if [ -d "/.runcvm/network/devices" ]; then
    for device_file in $(busybox ls /.runcvm/network/devices/* | busybox sort); do
      # Skip the 'default' symlink to avoid duplicates
      if [ "$(busybox basename "$device_file")" = "default" ]; then
        continue
      fi
      
      read DOCKER_IF DOCKER_IF_MAC DOCKER_IF_MTU DOCKER_IF_IP DOCKER_IF_IP_NETPREFIX DOCKER_IF_IP_GW < "$device_file"
      
      # For Firecracker, we need a unique MAC for the guest side.
      # We modify the container's MAC to start with AA:FC:00 so it's distinct but related.
      # If we have multiple interfaces, this simple sed might map them to the same MAC if the suffix is identical.
      # But typically container MACs are unique.
      local fc_mac=$(echo "$DOCKER_IF_MAC" | busybox sed 's/^..:..:../AA:FC:00/')
      
      local tap_name=""
      
      if [ "$is_host_mode" = "1" ] && [ "$DOCKER_IF" = "eth0" ]; then
        # Host Mode (NAT/TAP)
        # Entrypoint already created 'tap0' for us
        tap_name="tap0"
        log "  Network ($DOCKER_IF): Host Mode, using existing $tap_name"
        
        # Ensure it's up
        ip link set dev "$tap_name" up mtu "${DOCKER_IF_MTU:-1500}"
        
      else
        # Bridge Mode (Docker/Kubernetes)
        tap_name="tap-$DOCKER_IF"
        local bridge_name="br-$DOCKER_IF"
        
        log "  Network ($DOCKER_IF): $tap_name ($fc_mac) -> $bridge_name"
        
        # Create TAP device for Firecracker if it doesn't exist
        if ! ip link show "$tap_name" >/dev/null 2>&1; then
           log "    Creating TAP device $tap_name..."
           ip tuntap add dev "$tap_name" mode tap
        fi
        
        # Add TAP to the bridge
        log "    Adding $tap_name to bridge $bridge_name..."
        ip link set dev "$tap_name" master "$bridge_name"
        
        # Set MTU and bring up
        ip link set dev "$tap_name" up mtu "${DOCKER_IF_MTU:-1500}"
        
        # Add IP alias to bridge to allow container-to-VM communication (e.g. docker exec)
        # We pick 172.17.0.254 (or derived from network) to be on the same subnet as VM.
        # This ensures ARP works correctly without needing special routes in VM.
        # Note: If DOCKER_IF_IP_NETPREFIX=16, we use .254.
        # We need to construct a valid IP.
        # Hardcoding .254 might conflict if the subnet is small (/24).
        # But Docker default is /16.
        # Safer: Add address 169.254.1.1/32 to bridge here explicitly if not done elsewhere?
        # NO, we want SAME SUBNET.
        # Let's derive it. echo "172.17.255.254"
        # For now, simplistic approach: Force 169.254.1.1 is already there. Add 10.0.0.1?
        # If we use 169.254.1.1 on bridge and added route to VM, it SHOULD work.
        # Why did it fail?
        # Let's try adding the Docker Gateway IP (172.17.0.1) to the bridge? NO conflict.
        # Let's add 172.17.x.254.
        # Constructing IP logic is complex in shell.
        # I will stick to my "Route Injection" strategy BUT make sure it works.
        # Actually... if I simply add "ip addr add $DOCKER_IF_IP/32 ...".
        # No, that conflicts.
        #
        # Re-thinking: The route injection failed.
        # Let's try adding 169.254.1.2/32 to the VM side (in init script) as I planned before?
        # No I added route.
        # Let's try ADDING IP to Bridge: 172.17.255.254/16 (assuming /16).
        # If prefix is /24, then 172.17.0.254/24.
        
        # Implementation:
        net_prefix="${DOCKER_IF_IP_NETPREFIX:-16}"
        
        # Calculate dynamic alias IP to avoid hardcoding 172.17.255.254
        # We try to use the last IP in the subnet (Broadcast - 1) or similar.
        # Since we have minimal tools (busybox), we can use python enabled in image or ipcalc
        # 'ipcalc' in busybox: ipcalc -b 172.17.0.2/16 -> BROADCAST=172.17.255.255
        
        if command -v ipcalc >/dev/null 2>&1; then
           # Use Busybox ipcalc (wrap in || true to prevent set -e crash)
           local bcast=$(ipcalc -b "$DOCKER_IF_IP/$net_prefix" 2>/dev/null | cut -d= -f2 || true)
           if [ -n "$bcast" ]; then
              # Subtract 1 from broadcast using awk (sed might be missing)
              # If last octet is 255, make it 254. Otherwise subtract 1.
              # Trim potential whitespace from bcast
              alias_ip=$(echo "$bcast" | tr -d ' ' | awk -F. 'BEGIN{OFS="."} {if ($4==255) $4=254; else $4=$4-1; print}')
           else
              base_ip=$(echo "$DOCKER_IF_IP" | cut -d. -f1-3)
              alias_ip="${base_ip}.254"
           fi
        else
            base_ip=$(echo "$DOCKER_IF_IP" | cut -d. -f1-3)
            alias_ip="${base_ip}.254"
        fi

        # CLEANUP: Remove interfering SNAT rules and IPs that break routing
        log "    Cleaning up default SNAT rules and Link-Local IP..."
        # Remove bad SNAT for SSH port 22222
        if [ -n "$RUNCVM_GUEST" ] && [ -d "$RUNCVM_GUEST/usr/lib/xtables" ]; then
            XTABLES_LIBDIR=$RUNCVM_GUEST/usr/lib/xtables xtables-nft-multi iptables \
              -t nat -D POSTROUTING -p tcp -m tcp --dport 22222 -j SNAT --to-source 169.254.169.254 2>/dev/null || true
        else
            iptables -t nat -D POSTROUTING -p tcp -m tcp --dport 22222 -j SNAT --to-source 169.254.169.254 2>/dev/null || true
        fi
        
        # Remove Link-Local IP from bridge to force usage of our Alias
        ip addr del 169.254.1.1/32 dev "$bridge_name" 2>/dev/null || true
        
        log "    Adding connectivity alias $alias_ip to $bridge_name"
        if ip addr add "$alias_ip/$net_prefix" dev "$bridge_name"; then
            log "    Alias added successfully"
            
            # CRITICAL FIX: Flush the original Container IP from the bridge AND the interface.
            # If we keep it, the kernel treats traffic to VM (which has the same IP) 
            # as Local delivery, causing "Connection Refused" (or Host Unreachable loop).
            # By removing it, we force the kernel to route via the bridge (using the Alias for subnet).
            log "    Flushing conflicting container IP $DOCKER_IF_IP from $bridge_name and $DOCKER_IF..."
            if [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]; then ip addr show; fi
            ip addr del "$DOCKER_IF_IP/$net_prefix" dev "$bridge_name" 2>/dev/null || true
            ip addr del "$DOCKER_IF_IP/32" dev "$bridge_name" 2>/dev/null || true
            # Also flush from the slave interface (Docker might put it there)
            ip addr del "$DOCKER_IF_IP/$net_prefix" dev "$DOCKER_IF" 2>/dev/null || true
            ip addr del "$DOCKER_IF_IP/32" dev "$DOCKER_IF" 2>/dev/null || true
            
            # Restore Default Route (lost when flushing IPs)
            # This ensures the container namespace (host side) has internet access via the bridge
            if [ -n "$DOCKER_IF_IP_GW" ]; then
                log "    Restoring default route via $DOCKER_IF_IP_GW dev $bridge_name"
                ip route add default via "$DOCKER_IF_IP_GW" dev "$bridge_name" 2>/dev/null || true
            fi
        else
            log "    ERROR: Failed to add alias (Exit $?)"
        fi
      
        # Debug: show bridge and tap status
        if [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]; then
          log "    Bridge status:"
          ip link show "$bridge_name" 2>&1 | busybox sed 's/^/      /'
          log "    TAP status:"
          ip link show "$tap_name" 2>&1 | busybox sed 's/^/      /'
        fi
        
        # Add MASQUERADE rule for outbound traffic from the VM (for the default gateway interface)
        # This allows the VM to reach external networks (internet)
        if [ -n "$DOCKER_IF_IP" ] && [ "$DOCKER_IF_IP" != "-" ]; then
             log "    Adding NAT MASQUERADE rule for $DOCKER_IF..."
             if [ -n "$RUNCVM_GUEST" ] && [ -d "$RUNCVM_GUEST/usr/lib/xtables" ]; then
               XTABLES_LIBDIR=$RUNCVM_GUEST/usr/lib/xtables xtables-nft-multi iptables \
                 -t nat -A POSTROUTING -s "$DOCKER_IF_IP" -o "$DOCKER_IF_IP" -j MASQUERADE 2>/dev/null || true
                 # Note: The original code used -o eth0 (hardcoded). 
                 # In bridge mode, the traffic leaves via the container's eth0 usually?
                 # Actually, for the VM, traffic leaves via TAP -> Bridge -> Container Eth0.
                 # The NAT rule is needed if the *container* needs to NAT VM traffic out to the world.
                 # Originally: -s "$DOCKER_IF_IP" -o eth0.
                 # If this is eth0, then -o eth0 is correct (BUT wait, eth0 is now a slave of br-eth0).
                 # So traffic leaves via br-eth0? No, traffic leaves via the bridge routing?
                 #
                 # Wait, in the entrypoint, we moved the Container IP to br-eth0?
                 # Entrypoint: `ip addr flush dev eth0`, `ip addr add ... dev br-eth0`.
                 # So the container's "external" interface is now br-eth0.
                 #
                 # But we usually want to NAT traffic *leaving* the container infrastructure?
                 # Actually, usually Docker handles the NAT for the container.
                 # This rule seems to be for traffic *originating from the VM* (which has FC_IP which is DOCKER_IF_IP).
                 #
                 # Wait, the VM uses the SAME IP as the container (`DOCKER_IF_IP`).
                 # So effectively the VM *is* the container on the network.
                 # So we don't need NAT for the VM to be seen as the container IP.
                 #
                 # The original code:
                 # iptables -t nat -A POSTROUTING -s "$DOCKER_IF_IP" -o eth0 -j MASQUERADE
                 # This masquerades traffic FROM the IP (VM/Container IP) going out eth0.
                 # This looks weird. If source is IP, why masquerade?
                 # Maybe to fix some routing issue with the bridge?
                 #
                 # In multi-nic setup, if we have eth0 and eth1.
                 # We should probably skip this explicit NAT rule if we are confident the bridge routing works.
                 # But let's assume valid: if valid, it should likely be:
                 # -o eth0 (if that's the upstream).
                 # If we have multiple NICs, which is upstream?
                 # Usually the default gateway interface. DOCKER_IF_IP_GW tells us.
                 
                 # Let's keep it simple: if this interface has a Gateway, we add the rule.
                 if [ -n "$DOCKER_IF_IP_GW" ] && [ "$DOCKER_IF_IP_GW" != "-" ]; then
                    # This is likely the default route interface.
                    # The physical interface is $DOCKER_IF (now enslaved to br-$DOCKER_IF).
                    # But the *route* probably goes via ... wait.
                    # If we enslaved eth0 to br-eth0, traffic goes out via br-eth0?
                    # The bridge *is* the interface with the IP.
                    
                    # Let's keep the existing logic but make it dynamic. 
                    # Use -j MASQUERADE for all traffic from this IP leaving the container?
                    # Actually, let's just stick to the original "eth0" logic for "eth0" interface only for safety,
                    # or apply to all?
                    
                    # Original code was hardcoded to eth0.
                    # Currently we iterate. If DOCKER_IF == eth0, we can add it.
                    true
                 fi
             fi
        fi
      fi
      
      # Build JSON object for this interface
      local iface_json="{\"iface_id\":\"$DOCKER_IF\",\"guest_mac\":\"$fc_mac\",\"host_dev_name\":\"$tap_name\"}"
      
      if [ -z "$network_interfaces_json" ]; then
        network_interfaces_json="$iface_json"
      else
        network_interfaces_json="$network_interfaces_json,$iface_json"
      fi
      
    done
  fi # if devices dir exists

  # Wrap in array structure if we found interfaces
  if [ -n "$network_interfaces_json" ]; then
    network_config=",\"network-interfaces\":[$network_interfaces_json]"
  fi

  # Build vsock config for FUSE volumes
  # FORCE ENABLE vsock for debugging - always add the vsock device
  local vsock_config=',"vsock":{"guest_cid":3,"uds_path":"/run/firecracker.vsock"}'
  log "  vsock: FORCE ENABLED (debugging)"
  log "    guest_cid: 3"
  log "    uds_path: /run/firecracker.vsock"
  

  # Build balloon config if enabled
  # amount_mib: 0 means the balloon is empty (guest has full memory)
  # deflate_on_oom: true allows the guest to reclaim memory if it runs out
  # stats_polling_interval_s: 1 enables memory stats reporting
  local balloon_config=""
  if [ "${RUNCVM_ENABLE_BALLOON:-false}" = "true" ]; then
    local balloon_size="${RUNCVM_BALLOON_SIZE_MIB:-0}"
    log "  Memory Ballooning: ENABLED (Size: ${balloon_size}MiB)"
    balloon_config=',"balloon":{"amount_mib":'$balloon_size',"deflate_on_oom":true,"stats_polling_interval_s":1}'
  else
    log "  Memory Ballooning: DISABLED (default)"
  fi

  # Create Firecracker config file
  cat > "$FIRECRACKER_CONFIG" << CFGEOF
{
  "boot-source": {
    "kernel_image_path": "$kernel_path",
    "boot_args": "$boot_args"
  },
  "logger": {
    "log_path": "/dev/null",
    "level": "Error",
    "show_level": false,
    "show_log_origin": false
  },
  "drives": [
    {
      "drive_id": "rootfs",
      "path_on_host": "$rootfs_path",
      "is_root_device": true,
      "is_read_only": false
    }
  ],
    "machine-config": {
      "vcpu_count": $vcpu_count,
      "mem_size_mib": $mem_mb
    }${network_config}${vsock_config}${balloon_config}
}
CFGEOF

  # ALWAYS show Firecracker config for debugging vsock issue
  log "=== FIRECRACKER CONFIG (checking vsock) ==="
  busybox cat "$FIRECRACKER_CONFIG" | while read line; do log "  $line"; done
  log "=== END FIRECRACKER CONFIG ==="
  
  # Verify vsock is in config
  if busybox grep -q "vsock-device" "$FIRECRACKER_CONFIG"; then
    log "✓ vsock-device is PRESENT in Firecracker config"
  else
    log "✗ vsock-device is MISSING from Firecracker config!"
  fi

  # ============================================================
  # HOST TERMINAL CONFIGURATION
  # ============================================================
  # NOTE: We tried stty raw but it may trigger DSR queries.
  # The working test script runs Firecracker without modifying host terminal.
  # Let's try the same approach here.
  # ============================================================
  
  log "Starting Firecracker..."
  local fc_error_log=""

  if [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]; then
    # In DEBUG mode, show everything directly
    "$FIRECRACKER_BIN" --no-api --config-file "$FIRECRACKER_CONFIG"
  else
    # In non-DEBUG mode, suppress stderr (logs, banner) by redirecting to a temp file
    # This preserves stdout (console) interactivity while hiding the banner
    fc_error_log="/tmp/firecracker-error-$$.log"
    
    # Run Firecracker: stdout -> terminal (interactive), stderr -> log file
    "$FIRECRACKER_BIN" --no-api --config-file "$FIRECRACKER_CONFIG" 2> "$fc_error_log"
  fi
  
  local exit_code=$?

  # If failed and we have a log, show it
  if [ "$exit_code" -ne 0 ] && [ -n "$fc_error_log" ] && [ -f "$fc_error_log" ]; then
    cat "$fc_error_log" >&2
  fi
  
  # Cleanup temp log
  [ -n "$fc_error_log" ] && rm -f "$fc_error_log"
  


  log "Firecracker exited with code $exit_code"
  
  # Cleanup
  busybox rm -f "$rootfs_path" "$FIRECRACKER_CONFIG"
  
  return $exit_code
}

main "$@"
